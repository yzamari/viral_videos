"""
Video generator using AI and video editing libraries
"""
import os
import json
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Any
import random
import time
import google.generativeai as genai
from moviepy.editor import *
try:
    from moviepy.video.fx.fadein import fadein
    from moviepy.video.fx.fadeout import fadeout
    from moviepy.audio.fx.audio_fadein import audio_fadein
    from moviepy.audio.fx.audio_fadeout import audio_fadeout
except ImportError:
    # Fallback for different moviepy versions
    fadein = None
    fadeout = None
    audio_fadein = None
    audio_fadeout = None
from moviepy.video.tools.subtitles import SubtitlesClip
from PIL import Image, ImageDraw, ImageFont
import numpy as np
from gtts import gTTS
import tempfile
import uuid
import subprocess
import requests

from ..models.video_models import (
    VideoAnalysis, GeneratedVideoConfig, GeneratedVideo, 
    Platform, VideoCategory, Narrative, Feeling, Language, TTSVoice
)
from ..utils.logging_config import get_logger
from ..utils.exceptions import (
    GenerationFailedError, RenderingError, 
    StorageError, ContentPolicyViolation
)
from .director import Director

logger = get_logger(__name__)

class MockVeo2Client:
    """Mock Veo-2 client that generates realistic placeholder videos instead of colored backgrounds"""
    
    def __init__(self, output_dir: str):
        self.output_dir = output_dir
        
    def generate_video_clip(self, prompt: str, duration: float, clip_id: str) -> str:
        """Generate a mock video clip using FFmpeg with realistic content"""
        output_path = os.path.join(self.output_dir, f"veo2_clip_{clip_id}.mp4")
        
        try:
            # Create realistic mock video using FFmpeg with text and motion
            # This simulates what Veo-2 would generate - realistic baby/content videos
            cmd = [
                'ffmpeg', '-f', 'lavfi', '-i', 
                f'testsrc2=duration={duration}:size=1080x1920:rate=30',
                '-vf',
                f'drawtext=text=\'{prompt[:50]}...\':fontcolor=white:fontsize=40:box=1:boxcolor=black@0.8:boxborderw=5:x=(w-text_w)/2:y=(h-text_h)/2',
                '-c:v', 'libx264', '-preset', 'ultrafast', 
                '-pix_fmt', 'yuv420p', '-y', output_path
            ]
            
            subprocess.run(cmd, check=True, capture_output=True)
            logger.info(f"Generated mock Veo-2 clip: {output_path}")
            return output_path
            
        except subprocess.CalledProcessError as e:
            logger.error(f"FFmpeg error generating clip: {e}")
            # Fallback to moviepy if FFmpeg fails
            return self._generate_fallback_clip(prompt, duration, clip_id)
    
    def _generate_fallback_clip(self, prompt: str, duration: float, clip_id: str) -> str:
        """Fallback clip generation using MoviePy"""
        output_path = os.path.join(self.output_dir, f"veo2_clip_{clip_id}_fallback.mp4")
        
        # Create realistic animated background instead of solid color
        colors = [(255, 182, 193), (173, 216, 230), (255, 218, 185), (221, 160, 221)]  # Soft pastels
        bg_color = random.choice(colors)
        
        background = ColorClip(size=(1080, 1920), color=bg_color, duration=duration)
        
        # Create realistic content overlay
        content_text = f"Baby Content: {prompt[:30]}..."
        try:
            # Use PIL for text to avoid ImageMagick issues
            text_img = self._create_text_with_pil(content_text, (1080, 300))
            text_clip = ImageClip(text_img, duration=duration).set_position('center')
            final_clip = CompositeVideoClip([background, text_clip])
        except Exception as e:
            logger.warning(f"Text overlay failed: {e}, using background only")
            final_clip = background
        
        final_clip.write_videofile(output_path, codec='libx264', audio=False, verbose=False, logger=None)
        return output_path
    
    def _create_text_with_pil(self, text: str, size: Tuple[int, int]) -> np.ndarray:
        """Create text image using PIL to avoid ImageMagick issues"""
        img = Image.new('RGBA', size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(img)
        
        try:
            font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 36)
        except:
            font = ImageFont.load_default()
        
        # Get text size and center it
        bbox = draw.textbbox((0, 0), text, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]
        
        x = (size[0] - text_width) // 2
        y = (size[1] - text_height) // 2
        
        # Draw text with outline
        for offset in [(-2, -2), (-2, 2), (2, -2), (2, 2)]:
            draw.text((x + offset[0], y + offset[1]), text, font=font, fill=(0, 0, 0, 255))
        draw.text((x, y), text, font=font, fill=(255, 255, 255, 255))
        
        return np.array(img)

class VideoGenerator:
    """Generate viral videos based on analysis insights"""
    
    def __init__(self, api_key: str, output_dir: str = "outputs", 
                 script_model: str = "gemini-2.5-flash", 
                 refinement_model: str = "gemini-2.5-pro",
                 veo_model: str = "veo-2",
                 use_real_veo2: bool = True,
                 use_vertex_ai: bool = False,
                 vertex_project_id: str = None,
                 vertex_location: str = None,
                 vertex_gcs_bucket: str = None):
        genai.configure(api_key=api_key)
        self.api_key = api_key  # Store API key as instance attribute
        
        # Multi-model setup
        self.script_model = genai.GenerativeModel(script_model)
        self.refinement_model = genai.GenerativeModel(refinement_model)
        self.veo_model = veo_model  # Veo-2 for video generation
        self.use_real_veo2 = use_real_veo2
        self.use_vertex_ai = use_vertex_ai
        
        # Initialize Veo-2 client (Vertex AI, real Google AI Studio, or mock)
        if use_vertex_ai:
            try:
                from .vertex_ai_veo2_client import VertexAIVeo2Client
                self.veo_client = VertexAIVeo2Client(
                    project_id=vertex_project_id or "viralgen-464411",
                    location=vertex_location or "us-central1", 
                    gcs_bucket=vertex_gcs_bucket or "viral-veo2-results",
                    output_dir=output_dir
                )
                veo_type = "Vertex AI Veo-2 (Real)"
            except ImportError as e:
                logger.warning(f"Could not import Vertex AI client: {e}")
                logger.info("Falling back to Mock Veo-2 client")
                self.veo_client = MockVeo2Client(output_dir)
                veo_type = "Mock Veo-2 (Vertex AI fallback)"
        elif use_real_veo2:
            try:
                from .real_veo2_client import RealVeo2Client
                self.veo_client = RealVeo2Client(api_key, output_dir)
                veo_type = "Google AI Studio Veo-2"
            except ImportError as e:
                logger.warning(f"Could not import real Veo-2 client: {e}")
                logger.info("Falling back to Mock Veo-2 client")
                self.veo_client = MockVeo2Client(output_dir)
                veo_type = "Mock Veo-2 (Google AI fallback)"
        else:
            self.veo_client = MockVeo2Client(output_dir)
            veo_type = "Mock Veo-2"
        
        self.director = Director(api_key, script_model)  # Use Director for script writing
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # Create clips subdirectory for individual Veo-2 clips
        self.clips_dir = os.path.join(output_dir, "veo2_clips")
        os.makedirs(self.clips_dir, exist_ok=True)
        
        logger.info(f"VideoGenerator initialized with Gemini 2.5 Flash (script), Gemini 2.5 Pro (refinement), {veo_type} (video)")
        
    def generate_video_config(self, 
                            analyses: List[VideoAnalysis],
                            platform: Platform,
                            category: VideoCategory,
                            topic: Optional[str] = None) -> GeneratedVideoConfig:
        """
        Generate video configuration based on trend analysis
        
        Args:
            analyses: List of video analyses to learn from
            platform: Target platform for the video
            category: Content category
            topic: Optional specific topic to focus on
            
        Returns:
            GeneratedVideoConfig with all specifications
        """
        try:
            logger.info(f"Generating video config for {platform.value} - {category.value}")
            
            # Extract successful patterns from analyses
            patterns = self._extract_success_patterns(analyses)
            
            # Get optimal duration for platform
            duration = self._get_optimal_duration(platform)
            
            # Generate content specifications
            content_spec = self._generate_content_spec(patterns, platform, category, topic)
            
            # Let director decide on frame continuity
            continuity_decision = self.director.decide_frame_continuity(
                topic=content_spec['topic'],
                style=content_spec['style'],
                category=category,
                duration=duration,
                platform=platform
            )
            
            # Create config
            config = GeneratedVideoConfig(
                target_platform=platform,
                category=category,
                duration_seconds=duration,
                topic=content_spec['topic'],
                style=content_spec['style'],
                tone=content_spec['tone'],
                target_audience=content_spec['target_audience'],
                hook=content_spec['hook'],
                main_content=content_spec['main_content'],
                call_to_action=content_spec['call_to_action'],
                visual_style=content_spec['visual_style'],
                color_scheme=content_spec['color_scheme'],
                text_overlays=content_spec['text_overlays'],
                transitions=content_spec['transitions'],
                background_music_style=content_spec['music_style'],
                voiceover_style=content_spec.get('voiceover_style'),
                sound_effects=content_spec.get('sound_effects', []),
                inspired_by_videos=[a.video_id for a in analyses[:5]],
                predicted_viral_score=self._predict_viral_score(content_spec, patterns),
                narrative=content_spec.get('narrative', Narrative.NEUTRAL),
                feeling=content_spec.get('feeling', Feeling.FUNNY),
                primary_language=content_spec.get('primary_language', Language.ENGLISH),
                additional_languages=content_spec.get('additional_languages', []),
                tts_voice=content_spec.get('tts_voice', TTSVoice.AUTO),
                frame_continuity=continuity_decision['use_frame_continuity'],
                fallback_only=content_spec.get('fallback_only', False),
                image_only_mode=content_spec.get('image_only_mode', False),
                use_image_fallback=content_spec.get('use_image_fallback', True),
                images_per_second=content_spec.get('images_per_second', 2)
            )
            
            # Store continuity decision details for later use
            config._continuity_details = continuity_decision
            
            logger.info(f"Generated config with viral score: {config.predicted_viral_score:.2f}")
            logger.info(f"Frame continuity: {config.frame_continuity} (score: {continuity_decision['continuity_score']})")
        
            return config
            
        except Exception as e:
            logger.error(f"Failed to generate video config: {e}")
            raise GenerationFailedError("config_generation", str(e))
        
    def _extract_success_patterns(self, analyses: List[VideoAnalysis]) -> Dict:
        """Extract successful patterns from video analyses"""
        patterns = {
            'themes': [],
            'hooks': [],
            'success_factors': [],
            'emotional_tones': [],
            'engagement_rates': []
        }
        
        for analysis in analyses:
            if analysis.viral_score > 0.7:  # Focus on highly viral videos
                patterns['themes'].extend(analysis.content_themes)
                patterns['hooks'].append(analysis.hook_analysis)
                patterns['success_factors'].extend(analysis.success_factors)
                patterns['emotional_tones'].append(analysis.emotional_tone)
                patterns['engagement_rates'].append(analysis.engagement_rate)
                
        return patterns
    
    def _predict_viral_score(self, content_spec: dict, patterns: dict) -> float:
        """Predict viral score based on content and patterns"""
        try:
            # Base score
            base_score = 0.5
            
            # Boost for comedy/humor content
            if any(theme.lower() in str(content_spec).lower() for theme in ['comedy', 'humor', 'funny']):
                base_score += 0.2
            
            # Boost for trending themes  
            trending_themes = patterns.get('themes', [])
            for theme in trending_themes[:3]:  # Top 3 themes
                if theme.lower() in str(content_spec).lower():
                    base_score += 0.1
                    
            # Add randomness for variety
            import random
            base_score += random.uniform(-0.1, 0.1)
            
            # Clamp between 0 and 1
            return max(0.0, min(1.0, base_score))
            
        except Exception as e:
            logger.warning(f"Could not predict viral score: {e}")
            return 0.6  # Default decent score
        
    def _generate_content_spec(self, patterns: Dict, platform: Platform, 
                             category: VideoCategory, topic: Optional[str]) -> Dict:
        """Generate content specifications using AI with trending patterns"""
        
        # Extract trending insights
        trending_themes = patterns.get('themes', [])[:5]
        trending_hooks = patterns.get('hooks', [])[:3]
        trending_factors = patterns.get('success_factors', [])[:5]
        trending_tones = list(set(patterns.get('emotional_tones', [])))[:3]
        
        prompt = f"""
        Create a viral video specification that combines the user's topic with trending patterns.
        
        USER'S TOPIC: {topic or f'trending {category.value} content'}
        
        TRENDING PATTERNS TO INCORPORATE:
        - Hot themes: {', '.join(trending_themes)}
        - Viral hooks: {'; '.join([h for h in trending_hooks if h])}
        - Success factors: {', '.join(trending_factors)}
        - Winning tones: {', '.join(trending_tones)}
        
        REQUIREMENTS:
        1. Keep the user's core topic but enhance it with trending elements
        2. Use trending hooks and success factors to make it more viral
        3. Include specific trending phrases and themes that are working now
        4. Create a topic that rides current trends while staying relevant to user's intent
        
        Platform: {platform.value} | Category: {category.value}
        
        EXACT JSON FORMAT:
        {{
            "topic": "Enhanced topic that combines user intent with trending elements",
            "style": "video style incorporating trending patterns",
            "tone": "emotional tone from trending analysis", 
            "target_audience": "specific audience based on trends",
            "hook": "opening hook using trending patterns",
            "main_content": ["scene 1 with trending elements", "scene 2 with viral factors", "scene 3 with trending themes"],
            "call_to_action": "CTA incorporating trending success factors",
            "visual_style": "visual style based on successful patterns",
            "color_scheme": ["#trending_color1", "#trending_color2", "#accent"],
            "text_overlays": [
                {{"text": "trending phrase overlay 1", "timing": "0-3", "style": "bold"}},
                {{"text": "viral hook overlay 2", "timing": "6-9", "style": "normal"}},
                {{"text": "trending CTA overlay 3", "timing": "15-18", "style": "bold"}}
            ],
            "transitions": ["trending transition 1", "trending transition 2"],
            "music_style": "music style from successful videos",
            "voiceover_style": "trending voiceover style",
            "sound_effects": ["trending effect 1", "trending effect 2"],
            "realistic_audio": true,
            "narrative": "trending narrative style",
            "feeling": "trending emotional feeling"
        }}
        
        JSON Response:"""
        
        try:
            # Use Gemini 2.5 Flash for initial content generation
            response = self.script_model.generate_content(prompt)
            # Extract JSON from response safely
            content_spec = self._extract_json_safely(response.text)
            if content_spec:
                # Ensure realistic audio is enabled
                content_spec['realistic_audio'] = True
                # Map narrative and feeling to valid enum values
                narrative_map = {
                    'american': Narrative.PRO_AMERICAN_GOVERNMENT,
                    'technology': Narrative.PRO_TECHNOLOGY,
                    'environment': Narrative.PRO_ENVIRONMENT,
                    'education': Narrative.PRO_EDUCATION,
                    'health': Narrative.PRO_HEALTH,
                    'family': Narrative.PRO_FAMILY,
                    'soccer': Narrative.PRO_SOCCER,
                    'animal': Narrative.AGAINST_ANIMAL_ABUSE
                }
                feeling_map = {
                    'serious': Feeling.SERIOUS,
                    'funny': Feeling.FUNNY,
                    'cynical': Feeling.CYNICAL,
                    'inspirational': Feeling.INSPIRATIONAL,
                    'dramatic': Feeling.DRAMATIC,
                    'playful': Feeling.PLAYFUL,
                    'emotional': Feeling.EMOTIONAL,
                    'energetic': Feeling.ENERGETIC,
                    'calm': Feeling.CALM,
                    'excited': Feeling.ENERGETIC,
                    'engaging': Feeling.PLAYFUL,
                    'humorous': Feeling.FUNNY,
                    'relatable': Feeling.PLAYFUL
                }
                
                # Get narrative from content or default
                narrative_text = str(content_spec.get('narrative', 'neutral')).lower()
                for key, value in narrative_map.items():
                    if key in narrative_text:
                        content_spec['narrative'] = value
                        break
                else:
                    content_spec['narrative'] = Narrative.NEUTRAL
                
                # Get feeling from content or default
                feeling_text = str(content_spec.get('feeling', 'funny')).lower()
                for key, value in feeling_map.items():
                    if key in feeling_text:
                        content_spec['feeling'] = value
                        break
                else:
                    content_spec['feeling'] = Feeling.FUNNY
                    
                return content_spec
            else:
                # Fallback specification with trending elements
                return self._get_trending_fallback_spec(platform, category, topic, patterns)
        except Exception as e:
            logger.error(f"Error generating content spec: {e}")
            return self._get_trending_fallback_spec(platform, category, topic, patterns)
            
    def _get_trending_fallback_spec(self, platform: Platform, category: VideoCategory, topic: str, patterns: Dict) -> Dict:
        """Get fallback content specification incorporating trending patterns"""
        trending_themes = patterns.get('themes', [])[:3]
        trending_tones = patterns.get('emotional_tones', ['energetic'])
        
        # Enhance user topic with trending elements
        enhanced_topic = topic
        if trending_themes:
            enhanced_topic = f"{topic}: {trending_themes[0].title()} Edition!"
        
        return {
            "topic": enhanced_topic,
            "style": "fast-paced with trending elements",
            "tone": trending_tones[0] if trending_tones else "energetic",
            "target_audience": "18-34 trending content consumers",
            "hook": f"You won't believe this {topic.lower()} discovery!",
            "main_content": [
                f"Hook with {topic} surprise",
                f"Main {topic} content with trending twist", 
                f"Viral {topic} moment",
                "Trending call to action"
            ],
            "call_to_action": "Follow for more trending content!",
            "visual_style": "bright, trending-style visuals",
            "color_scheme": ["#FF6B6B", "#4ECDC4", "#FFFFFF"],
            "text_overlays": [
                {"text": f"üî• {topic.upper()} ALERT!", "timing": "0-3", "style": "bold"},
                {"text": "This is going VIRAL! üìà", "timing": "6-9", "style": "normal"},
                {"text": "FOLLOW for daily trends! üëÜ", "timing": "15-20", "style": "bold"}
            ],
            "transitions": ["trending zoom", "viral fade"],
            "music_style": "trending upbeat",
            "voiceover_style": "energetic trending style",
            "sound_effects": ["viral whoosh", "trending pop"],
            "realistic_audio": True,
            "narrative": Narrative.NEUTRAL,
            "feeling": Feeling.ENERGETIC
        }
            
    def _get_fallback_spec(self, platform: Platform, category: VideoCategory) -> Dict:
        """Get fallback content specification"""
        return {
            "topic": f"Trending {category.value} content",
            "style": "fast-paced",
            "tone": "energetic",
            "target_audience": "18-34 year olds",
            "hook": "You won't believe what happens next!",
            "main_content": [
                "Opening scene with hook",
                "Main content delivery",
                "Climax or reveal",
                "Call to action"
            ],
            "call_to_action": "Follow for more!",
            "visual_style": "bright and colorful",
            "color_scheme": ["#FF6B6B", "#4ECDC4", "#FFFFFF"],
            "text_overlays": [
                {"text": "MUST WATCH!", "timing": "0-2", "style": "bold"},
                {"text": "FOLLOW FOR MORE", "timing": "28-30", "style": "bold"}
            ],
            "transitions": ["fade", "zoom"],
            "music_style": "upbeat electronic",
            "voiceover_style": "energetic and friendly",
            "sound_effects": ["whoosh", "pop"]
        }
        
    def generate_video(self, config: GeneratedVideoConfig) -> GeneratedVideo:
        """
        Generate actual video file based on configuration with real Veo-2 clips
        
        Args:
            config: Video configuration
            
        Returns:
            GeneratedVideo object with file information
        """
        start_time = datetime.now()
        video_id = str(uuid.uuid4())
        
        # Store current config for emotional TTS with trending elements
        self._current_config = {
            'narrative': getattr(config, 'narrative', 'engaging'),
            'feeling': getattr(config, 'feeling', 'excited'),
            'realistic_audio': getattr(config, 'realistic_audio', True)
        }
        
        # Create session folder for this generation
        session_name = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{video_id[:8]}"
        session_dir = os.path.join(self.output_dir, session_name)
        os.makedirs(session_dir, exist_ok=True)
        
        # Update output directories to use session folder
        original_output_dir = self.output_dir
        original_clips_dir = self.clips_dir
        self.output_dir = session_dir
        self.clips_dir = os.path.join(session_dir, "clips")
        os.makedirs(self.clips_dir, exist_ok=True)
        
        logger.info(f"üìÅ Created session folder: {session_name}")
        
        try:
            # Generate creative script with randomness
            script = self._generate_creative_script(config, video_id)
            
            # Generate Veo-2 prompts for realistic video
            veo_prompts = self._generate_veo2_prompts(config, script)
            
            # Save all prompts to files
            self._save_prompts_to_files(script, veo_prompts, video_id, config)
            
            # Generate individual Veo-2 video clips
            veo_clips = self._generate_veo2_clips(veo_prompts, config, video_id)
            
            # Generate natural voiceover with trending elements and exact duration matching
            voiceover_config = {
                'narrative': getattr(config, 'narrative', 'engaging'),
                'feeling': getattr(config, 'feeling', 'excited'),
                'realistic_audio': getattr(config, 'realistic_audio', True),
                'duration_seconds': config.duration_seconds
            }
            audio_path = self._generate_voiceover(script, config.duration_seconds, voiceover_config)
                
            # Compose final video with real Veo-2 clips
            video_path = self._compose_video_with_veo_clips(
                veo_clips, 
                audio_path, 
                config,
                video_id
            )
            
            # Get file size
            file_size_mb = os.path.getsize(video_path) / (1024 * 1024)
            
            generation_time = (datetime.now() - start_time).total_seconds()
            
            # Create video analysis file for this session
            self._create_video_analysis_file(config, script, veo_clips, video_id, generation_time, session_dir)
            
            generated_video = GeneratedVideo(
                video_id=video_id,
                config=config,
                file_path=video_path,
                file_size_mb=file_size_mb,
                generation_time_seconds=generation_time,
                ai_models_used=['gemini-2.5-flash', 'gemini-2.5-pro', 'mock-veo-2', 'gtts'],
                script=script,
                scene_descriptions=[clip['description'] for clip in veo_clips],
                audio_transcript=self._clean_script_for_tts(script, config.duration_seconds)
            )
            
            logger.info(f"‚úÖ Successfully generated video {video_id} in session folder: {session_name}")
            return generated_video
            
        except Exception as e:
            logger.error(f"Error generating video: {e}")
            raise
        finally:
            # Restore original output directories
            self.output_dir = original_output_dir
            self.clips_dir = original_clips_dir
    
    def _generate_creative_script(self, config: GeneratedVideoConfig, video_id: str) -> str:
        """Generate strict, influencer-style script with Gemini 2.5 Flash for authentic, engaging content"""
        
        # VARIETY FIX: Use different prompt templates to avoid repetitive phrases
        import random
        
        # Multiple hook variations to avoid repetition
        hook_variations = [
            ["Hold up", "Stop scrolling", "Listen up", "Check this out", "Real talk"],
            ["You know what's wild?", "Here's something crazy", "This is insane", "Get this", "No way"],
            ["I just found out", "Someone told me", "Did you know", "Plot twist", "Breaking news"],
            ["This is actually", "Honestly", "For real", "Not gonna lie", "Straight up"],
            ["Y'all", "Everyone", "People", "Guys", "Friends"]
        ]
        
        # Multiple conversation starters
        conversation_starters = [
            ["Let me tell you", "Here's the thing", "So basically", "Picture this", "Imagine"],
            ["What happened was", "The story goes", "Long story short", "To put it simply", "In other words"],
            ["The crazy part is", "But here's the twist", "Plot twist", "The best part", "What's wild is"],
            ["I couldn't believe", "It blew my mind", "I was shook", "This changed everything", "I had to share"],
            ["Everyone needs to know", "This is going viral", "People are talking", "The internet is crazy", "This is trending"]
        ]
        
        # Multiple reaction phrases  
        reaction_phrases = [
            ["Wait, what?", "Hold on", "Pause", "Stop right there", "Hang on"],
            ["This is insane", "This is crazy", "This is wild", "This is unreal", "This is nuts"],
            ["You won't believe", "I can't even", "This is too much", "I'm dead", "I'm shook"],
            ["For real?", "Are you serious?", "No cap", "I'm not kidding", "This is real"],
            ["Mind blown", "I'm speechless", "This hits different", "Game changer", "Life changing"]
        ]
        
        # Randomly select variations for this video
        selected_hooks = [random.choice(group) for group in hook_variations]
        selected_starters = [random.choice(group) for group in conversation_starters]
        selected_reactions = [random.choice(group) for group in reaction_phrases]
        
        # Create varied influencer-style template-based prompt
        influencer_prompt = f"""
You are a viral content creator with millions of followers. Create a {config.duration_seconds}-second script about: {config.topic}

INFLUENCER STYLE REQUIREMENTS (CRITICAL - FOLLOW EXACTLY):
- Write like you're personally talking to your best friend
- Use ONLY **VOICEOVER:** lines for spoken content
- Sound conversational, authentic, and relatable
- Hook viewers in the first 3 seconds with intrigue/surprise

VARIETY REQUIREMENT - USE THESE SPECIFIC PHRASES (NOT THE USUAL ONES):
Hook starters: {', '.join(selected_hooks)}
Conversation starters: {', '.join(selected_starters)}
Reaction phrases: {', '.join(selected_reactions)}

AVOID OVERUSED PHRASES:
- Don't always start with "OK" or "Okay"
- Don't always use "Wait!" - mix it up
- Don't repeat "you guys" - use variety
- Don't always say "This is actually crazy" - be creative

ENGAGEMENT HOOKS (USE VARIED APPROACHES):
- Curiosity: "{selected_hooks[1]}? {selected_starters[0]}"
- Urgency: "{selected_hooks[0]}, {selected_starters[1]}"
- Personal: "{selected_starters[2]} something that {selected_reactions[3]}"
- Relatable: "{selected_hooks[4]}, {selected_starters[4]}"

CONTENT STRUCTURE:
HOOK (3 seconds): Grab attention with intrigue using selected phrases
MAIN CONTENT ({config.duration_seconds-6} seconds): Develop the story naturally
CTA/OUTRO (3 seconds): Personal call-to-action

SCRIPT FORMAT:
**VOICEOVER:** [Your authentic, conversational dialogue here using selected phrases]
**VOICEOVER:** [Continue the authentic conversation with variety]
**VOICEOVER:** [Natural, engaging conclusion]

Generate a script that sounds fresh and avoids repetitive viral phrases. Use the selected phrase variations to create unique content.
"""
        
        try:
            logger.info("üé≠ Generating varied influencer-style script with Gemini 2.5 Flash...")
            
            # Use only Gemini 2.5 Flash for fast, reliable generation
            response = self.script_model.generate_content(influencer_prompt)
            
            if not response or not response.text:
                logger.warning("Empty response from script model, using fallback")
                return self._generate_fallback_influencer_script(config)
            
            script = response.text.strip()
            
            # Validate the script format
            if "**VOICEOVER:**" not in script:
                logger.warning("Generated script missing VOICEOVER format, fixing...")
                script = self._fix_script_format(script)
            
            # Log successful generation
            logger.info(f"‚úÖ Generated {len(script)} character varied influencer script")
            logger.info(f"üé¨ Script preview: {script[:100]}...")
            
            return script
            
        except Exception as e:
            logger.error(f"Script generation failed: {e}")
            logger.info("üîÑ Using fallback influencer script...")
            return self._generate_fallback_influencer_script(config)
    
    def _generate_fallback_influencer_script(self, config: GeneratedVideoConfig) -> str:
        """Generate guaranteed influencer-style fallback script"""
        
        # Create authentic influencer content based on topic
        topic_lower = config.topic.lower()
        
        # Influencer hooks based on topic type
        if any(word in topic_lower for word in ['news', 'breaking', 'update', 'happened']):
            hook = "Wait, did you guys see what just happened? I had to come on here immediately because"
            main = f"this whole {config.topic} situation is actually insane. Let me break down what's really going on here."
            outro = "Drop your thoughts below because I need to know if you're seeing this too!"
            
        elif any(word in topic_lower for word in ['secret', 'hidden', 'truth', 'revealed']):
            hook = "Y'all are not ready for what I just discovered. I'm literally shaking because"
            main = f"nobody's talking about {config.topic} and honestly it's changing everything."
            outro = "Save this before it gets taken down. Trust me on this one!"
            
        elif any(word in topic_lower for word in ['tips', 'how to', 'advice', 'tutorial']):
            hook = "I can't be the only one who didn't know this, but"
            main = f"this {config.topic} hack just changed my entire life and I had to share."
            outro = "Try this and let me know if it works for you too!"
            
        elif any(word in topic_lower for word in ['drama', 'controversy', 'scandal']):
            hook = "The internet is going crazy over this and honestly"
            main = f"the {config.topic} situation has me speechless. Here's what actually happened."
            outro = "What side are you on? Because the comments are about to be wild!"
            
        else:
            # Generic influencer approach
            hook = "You know what's actually wild that nobody talks about?"
            main = f"This whole {config.topic} thing hits different when you really think about it."
            outro = "Am I the only one who feels this way? Let me know in the comments!"
        
        # Build complete script
        script = f"""**VOICEOVER:** {hook} {main}

**VOICEOVER:** {outro}"""
        
        logger.info(f"üé≠ Generated fallback influencer script: {len(script)} characters")
        return script
    
    def _fix_script_format(self, script: str) -> str:
        """Fix script to use proper VOICEOVER format"""
        
        # If it's plain text, wrap it in VOICEOVER format
        if "**VOICEOVER:**" not in script:
            # Split into natural segments for better flow
            lines = script.split('\n')
            formatted_lines = []
            
            for line in lines:
                line = line.strip()
                if line:
                    if not line.startswith("**"):
                        line = f"**VOICEOVER:** {line}"
                    formatted_lines.append(line)
            
            script = '\n\n'.join(formatted_lines)
        
        return script

    def _generate_veo2_clips(self, veo_prompts: List[Dict], config: GeneratedVideoConfig, video_id: str) -> List[Dict]:
        """Generate optimized Veo-2/3 clips with quota management and 8-second optimization"""
        
        # Check if fallback-only mode is enabled
        fallback_only = getattr(config, 'fallback_only', False)
        image_only_mode = getattr(config, 'image_only_mode', False)
        
        if image_only_mode:
            logger.info(f"üñºÔ∏è IMAGE-ONLY MODE ENABLED")
            logger.info(f"üì∏ Generating AI images instead of videos")
            logger.info(f"‚ú® Creating coherent visual story with script")
            return self._generate_image_based_fallback_clips(veo_prompts, config, video_id)
        
        if fallback_only:
            logger.info(f"üé® FALLBACK-ONLY MODE ENABLED")
            logger.info(f"üìù Generating descriptive video fallbacks (no VEO2/VEO3 attempts)")
            logger.info(f"‚úÖ Perfect for testing without quota issues")
            return self._generate_descriptive_fallback_clips(veo_prompts, config, video_id)
        
        logger.info(f"üéØ QUOTA-OPTIMIZED VEO GENERATION")
        logger.info(f"üìä Google AI Tier 1 Limits: 2 videos/minute, 50 videos/day")
        logger.info(f"üí° Using 8-second clips and Veo-2 ‚Üí Veo-3 ‚Üí Image ‚Üí Fallback chain")
        
        try:
            # Try optimized Veo client first
            from .optimized_veo_client import OptimizedVeoClient
            api_key = getattr(self, 'api_key', os.getenv('GOOGLE_API_KEY', os.getenv('GEMINI_API_KEY')))
            optimized_client = OptimizedVeoClient(api_key, self.output_dir)
            
            # üé≠ ENHANCED ORCHESTRATION: Coordinate script and video generation
            actual_duration = int(os.getenv('VIDEO_DURATION', config.duration_seconds))
            logger.info(f"üéØ ORCHESTRATED: Using duration: {actual_duration}s")
            
            # DIRECTOR AGENT: Ensure script matches video clip count and timing
            expected_clips = len(veo_prompts)
            expected_total_duration = expected_clips * 8  # 8s per clip
            
            logger.info(f"üé≠ DIRECTOR ORCHESTRATION:")
            logger.info(f"   Expected clips: {expected_clips}")
            logger.info(f"   Expected duration: {expected_total_duration}s")
            logger.info(f"   Target duration: {actual_duration}s")
            
            # VIDEO GENERATOR AGENT: Generate clips with orchestrated timing
            veo_clips = optimized_client.generate_optimized_clips(
                prompts=veo_prompts,
                config={
                    'duration_seconds': expected_total_duration,  # Use expected duration
                    'platform': config.target_platform,
                    'frame_continuity': getattr(config, 'frame_continuity', False),
                    'orchestrated': True  # Flag for orchestrated generation
                },
                video_id=video_id
            )
            
            # Convert to expected format
            formatted_clips = []
            for clip in veo_clips:
                formatted_clip = {
                    'clip_path': clip['clip_path'],
                    'description': clip['description'],
                    'veo2_prompt': clip['veo2_prompt'],
                    'duration': clip['duration'],
                    'scene_index': clip['scene_index'],
                    'file_size_mb': clip['file_size_mb'],
                    'generated_with': clip['generated_with'],
                    'optimized': True
                }
                formatted_clips.append(formatted_clip)
            
            logger.info(f"üé¨ Optimized generation SUCCESS: {len(formatted_clips)} clips")
            return formatted_clips
            
        except Exception as optimized_error:
            logger.warning(f"‚ö†Ô∏è Optimized client failed: {optimized_error}")
            
            # Check if we should try image-based fallback
            use_image_fallback = getattr(config, 'use_image_fallback', True)
            
            if use_image_fallback and "quota" in str(optimized_error).lower():
                logger.info("üñºÔ∏è VEO quota exhausted - switching to AI image generation")
                return self._generate_image_based_fallback_clips(veo_prompts, config, video_id)
            
            logger.info("üîÑ Falling back to standard generation with 8s limit...")
            
            # Fallback to standard generation with optimizations
            # STEP 1: Review and refine prompts with Gemini 2.5 Pro
            refined_prompts = self._review_and_refine_veo2_prompts(veo_prompts, config)
            
            # STEP 2: Generate clips with 8-second optimization
            veo_clips = []
            
            # OPTIMIZATION: Use 8-second clips (max supported duration)
            optimal_clip_duration = min(8.0, config.duration_seconds / len(refined_prompts))
            num_clips = max(1, int(config.duration_seconds / 8.0))  # Fewer, longer clips
            
            logger.info(f"üé¨ Standard 8s clip generation: {num_clips} clips of {optimal_clip_duration:.1f}s each")
            logger.info(f"üìÅ Clips will be saved in: {self.clips_dir}")
            
            # Ensure clips directory exists
            os.makedirs(self.clips_dir, exist_ok=True)
            
            for i in range(min(num_clips, len(refined_prompts))):
                prompt_data = refined_prompts[i]
                clip_id = f"{video_id}_scene_{i}"
                
                try:
                    logger.info(f"üé• Generating 8s clip {i+1}/{num_clips}: {clip_id}")
                    logger.info(f"üìù Refined prompt: {prompt_data['veo2_prompt'][:100]}...")
                    
                    # Generate individual Veo-2 clip using refined prompt (8 seconds max)
                    clip_path = self.veo_client.generate_video_clip(
                        prompt_data['veo2_prompt'], 
                        optimal_clip_duration,  # Use 8-second optimization
                        clip_id
                    )
                    
                    # Check if clip was actually created
                    if clip_path and os.path.exists(clip_path):
                        file_size = os.path.getsize(clip_path) / (1024 * 1024)  # MB
                        logger.info(f"‚úÖ Successfully generated 8s clip {i+1}: {clip_path} ({file_size:.1f}MB)")
                        
                        clip_info = {
                            'clip_path': clip_path,
                            'description': prompt_data['description'],
                            'veo2_prompt': prompt_data['veo2_prompt'],
                            'original_prompt': veo_prompts[i]['veo2_prompt'] if i < len(veo_prompts) else prompt_data['veo2_prompt'],
                            'duration': optimal_clip_duration,
                            'scene_index': i,
                            'file_size_mb': file_size,
                            'reviewed_by': prompt_data.get('reviewed_by', 'none'),
                            'optimized_8s': True
                        }
                        
                        veo_clips.append(clip_info)
                    else:
                        logger.error(f"‚ùå Clip generation returned invalid path: {clip_path}")
                        raise Exception(f"Invalid clip path returned: {clip_path}")
                    
                    # QUOTA MANAGEMENT: Wait between generations
                    if i < num_clips - 1:  # Don't wait after last clip
                        logger.info("‚è∞ Quota management: waiting 30s before next generation...")
                        time.sleep(30)
                    
                except Exception as e:
                    logger.error(f"‚ùå Failed to generate Veo-2 clip {i+1}: {e}")
                    
                    # Check if it's a quota error
                    if "quota" in str(e).lower() or "429" in str(e):
                        logger.info(f"üñºÔ∏è Quota exhausted - switching to image-based fallback for remaining clips")
                        # Generate image-based clips for the rest
                        remaining_prompts = refined_prompts[i:]
                        remaining_duration = optimal_clip_duration * len(remaining_prompts)
                        
                        image_clips = self._generate_image_based_fallback_clips(
                            remaining_prompts, 
                            GeneratedVideoConfig(
                                topic=config.topic,
                                duration_seconds=remaining_duration,
                                target_platform=config.target_platform,
                                video_category=config.video_category,
                                predicted_viral_score=config.predicted_viral_score
                            ),
                            video_id
                        )
                        
                        # Adjust scene indices
                        for img_clip in image_clips:
                            img_clip['scene_index'] += i
                        
                        veo_clips.extend(image_clips)
                        break
                    else:
                        logger.info(f"üîÑ Creating fallback clip for scene {i+1}")
                        
                        # Create fallback clip
                        try:
                            fallback_path = self._create_fallback_clip(prompt_data, optimal_clip_duration, clip_id)
                            if fallback_path and os.path.exists(fallback_path):
                                file_size = os.path.getsize(fallback_path) / (1024 * 1024)
                                logger.info(f"üÜò Created 8s fallback clip: {fallback_path} ({file_size:.1f}MB)")
                                
                                clip_info = {
                                    'clip_path': fallback_path,
                                    'description': prompt_data['description'],
                                    'veo2_prompt': prompt_data['veo2_prompt'],
                                    'original_prompt': veo_prompts[i]['veo2_prompt'] if i < len(veo_prompts) else prompt_data['veo2_prompt'],
                                    'duration': optimal_clip_duration,
                                    'scene_index': i,
                                    'file_size_mb': file_size,
                                'is_fallback': True,
                                'reviewed_by': prompt_data.get('reviewed_by', 'none'),
                                'optimized_8s': True
                            }
                            veo_clips.append(clip_info)
                        else:
                            logger.error(f"‚ùå Even fallback clip creation failed for scene {i+1}")
                            continue
                    except Exception as fallback_error:
                        logger.error(f"‚ùå Fallback clip creation failed: {fallback_error}")
                        continue
            
            logger.info(f"üé¨ Standard 8s generation complete: {len(veo_clips)} clips created")
            
            # Log details of each generated clip
            for i, clip in enumerate(veo_clips):
                logger.info(f"  Clip {i+1}: {os.path.basename(clip['clip_path'])} - {clip['file_size_mb']:.1f}MB")
                
            return veo_clips
    
    def _generate_descriptive_fallback_clips(self, veo_prompts: List[Dict], config: GeneratedVideoConfig, video_id: str) -> List[Dict]:
        """Generate descriptive fallback clips with detailed scene descriptions"""
        logger.info(f"üé® DESCRIPTIVE FALLBACK MODE")
        logger.info(f"üìù Creating videos with scene descriptions (no VEO quota usage)")
        
        clips = []
        duration_per_clip = config.duration_seconds / len(veo_prompts)
        
        for i, prompt_data in enumerate(veo_prompts):
            clip_id = f"{video_id}_fallback_scene_{i}"
            
            try:
                # Create descriptive fallback clip
                clip_path = self._create_descriptive_fallback_clip(
                    prompt_data, 
                    duration_per_clip, 
                    clip_id,
                    config
                )
                
                clip_info = {
                    'clip_path': clip_path,
                    'description': prompt_data['description'],
                    'veo2_prompt': prompt_data['veo2_prompt'],
                    'duration': duration_per_clip,
                    'scene_index': i,
                    'generated_with': 'descriptive_fallback',
                    'fallback_type': 'descriptive_text'
                }
                
                clips.append(clip_info)
                logger.info(f"‚úÖ Descriptive fallback clip {i+1}/{len(veo_prompts)} complete")
                
            except Exception as e:
                logger.error(f"Failed to create descriptive fallback clip {i}: {e}")
                continue
        
        return clips

    def _generate_image_based_fallback_clips(self, veo_prompts: List[Dict], config: GeneratedVideoConfig, video_id: str) -> List[Dict]:
        """Generate image-based fallback clips using Google's image generation models"""
        logger.info(f"üñºÔ∏è IMAGE-BASED FALLBACK MODE")
        logger.info(f"üì∏ Creating videos from AI-generated images (coherent with script)")
        
        clips = []
        duration_per_clip = config.duration_seconds / len(veo_prompts)
        images_per_second = 2  # 2 images per second for smooth transitions
        
        for i, prompt_data in enumerate(veo_prompts):
            clip_id = f"{video_id}_image_scene_{i}"
            
            try:
                # Generate images for this scene
                scene_images = self._generate_scene_images(
                    prompt_data,
                    duration_per_clip,
                    images_per_second,
                    clip_id
                )
                
                if scene_images:
                    # Create video from images
                    clip_path = self._create_video_from_images(
                        scene_images,
                        duration_per_clip,
                        clip_id,
                        prompt_data
                    )
                    
                    clip_info = {
                        'clip_path': clip_path,
                        'description': prompt_data['description'],
                        'veo2_prompt': prompt_data['veo2_prompt'],
                        'duration': duration_per_clip,
                        'scene_index': i,
                        'generated_with': 'image_based_fallback',
                        'fallback_type': 'ai_images',
                        'images_count': len(scene_images),
                        'images_per_second': images_per_second
                    }
                    
                    clips.append(clip_info)
                    logger.info(f"‚úÖ Image-based clip {i+1}/{len(veo_prompts)} complete ({len(scene_images)} images)")
                else:
                    logger.warning(f"No images generated for scene {i}, falling back to descriptive")
                    # Fallback to descriptive if image generation fails
                    clip_path = self._create_descriptive_fallback_clip(
                        prompt_data, 
                        duration_per_clip, 
                        clip_id,
                        config
                    )
                    
                    clip_info = {
                        'clip_path': clip_path,
                        'description': prompt_data['description'],
                        'veo2_prompt': prompt_data['veo2_prompt'],
                        'duration': duration_per_clip,
                        'scene_index': i,
                        'generated_with': 'descriptive_fallback',
                        'fallback_type': 'descriptive_text'
                    }
                    clips.append(clip_info)
                
            except Exception as e:
                logger.error(f"Failed to create image-based clip {i}: {e}")
                # Final fallback to descriptive
                try:
                    clip_path = self._create_descriptive_fallback_clip(
                        prompt_data, 
                        duration_per_clip, 
                        clip_id,
                        config
                    )
                    
                    clip_info = {
                        'clip_path': clip_path,
                        'description': prompt_data['description'],
                        'veo2_prompt': prompt_data['veo2_prompt'],
                        'duration': duration_per_clip,
                        'scene_index': i,
                        'generated_with': 'descriptive_fallback',
                        'fallback_type': 'descriptive_text'
                    }
                    clips.append(clip_info)
                except Exception as final_e:
                    logger.error(f"All fallback methods failed for clip {i}: {final_e}")
                    continue
        
        return clips

    def _generate_scene_images(self, prompt_data: Dict, duration: float, images_per_second: int, clip_id: str) -> List[str]:
        """Generate AI images for a scene using Google's image generation models"""
        try:
            import google.generativeai as genai
            
            # Configure the image generation model
            genai.configure(api_key=self.api_key)
            
            # Calculate number of images needed
            total_images = max(1, int(duration * images_per_second))
            logger.info(f"üé® Generating {total_images} images for {duration:.1f}s scene")
            
            # Create coherent image prompts for the scene
            base_prompt = prompt_data.get('veo2_prompt', prompt_data.get('description', 'scene'))
            
            # Enhance for image generation
            image_prompts = self._create_coherent_image_prompts(base_prompt, total_images)
            
            generated_images = []
            images_dir = os.path.join(self.output_dir, "scene_images", clip_id)
            os.makedirs(images_dir, exist_ok=True)
            
            for i, img_prompt in enumerate(image_prompts):
                try:
                    logger.info(f"üñºÔ∏è Generating image {i+1}/{total_images}: {img_prompt[:50]}...")
                    
                    # Try Google's Imagen model
                    image_path = self._generate_single_image(img_prompt, images_dir, f"img_{i:03d}")
                    
                    if image_path and os.path.exists(image_path):
                        generated_images.append(image_path)
                        logger.info(f"‚úÖ Image {i+1} generated: {image_path}")
                    else:
                        logger.warning(f"‚ö†Ô∏è Image {i+1} generation failed, creating placeholder")
                        # Create a placeholder image with text
                        placeholder_path = self._create_placeholder_image(img_prompt, images_dir, f"placeholder_{i:03d}")
                        if placeholder_path:
                            generated_images.append(placeholder_path)
                    
                    # Small delay to avoid rate limits
                    time.sleep(1)
                    
                except Exception as e:
                    logger.warning(f"Image {i+1} generation failed: {e}")
                    # Create placeholder
                    placeholder_path = self._create_placeholder_image(img_prompt, images_dir, f"placeholder_{i:03d}")
                    if placeholder_path:
                        generated_images.append(placeholder_path)
            
            logger.info(f"üé® Generated {len(generated_images)}/{total_images} images for scene")
            return generated_images
            
        except ImportError:
            logger.error("Google AI SDK not available for image generation")
            return []
        except Exception as e:
            logger.error(f"Scene image generation failed: {e}")
            return []

    def _create_coherent_image_prompts(self, base_prompt: str, num_images: int) -> List[str]:
        """Create coherent image prompts that tell a visual story"""
        
        # Define progression keywords for visual storytelling
        progression_keywords = [
            "establishing shot, wide view",
            "medium shot, focused view", 
            "close-up, detailed view",
            "action shot, dynamic view",
            "reaction shot, emotional view",
            "final shot, conclusive view"
        ]
        
        # Visual style keywords
        style_keywords = [
            "cinematic lighting, professional photography",
            "vibrant colors, high contrast", 
            "dramatic shadows, artistic composition",
            "natural lighting, realistic style",
            "dynamic angle, engaging perspective"
        ]
        
        prompts = []
        
        for i in range(num_images):
            # Select progression and style based on position in sequence
            prog_idx = min(i, len(progression_keywords) - 1)
            style_idx = i % len(style_keywords)
            
            progression = progression_keywords[prog_idx]
            style = style_keywords[style_idx]
            
            # Create enhanced prompt
            enhanced_prompt = f"{base_prompt}, {progression}, {style}, high quality, detailed"
            
            # Clean up the prompt
            enhanced_prompt = enhanced_prompt.replace("baby", "young person").replace("toddler", "small child")
            
            prompts.append(enhanced_prompt)
        
        return prompts

    def _generate_single_image(self, prompt: str, output_dir: str, filename: str) -> Optional[str]:
        """Generate a single image using Google's image generation API"""
        try:
            output_path = os.path.join(output_dir, f"{filename}.jpg")
            
            # Try Vertex AI Imagen first if enabled
            if os.getenv("ENABLE_VERTEX_AI_IMAGEN", "false").lower() == "true":
                try:
                    from src.generators.vertex_imagen_client import VertexImagenClient
                    
                    logger.info("üé® Attempting to use Vertex AI Imagen...")
                    imagen_client = VertexImagenClient()
                    
                    if imagen_client.initialized:
                        result = imagen_client.generate_image(
                            prompt=prompt,
                            output_path=output_path,
                            aspect_ratio="16:9"
                        )
                        
                        if result:
                            logger.info(f"‚úÖ Successfully generated image with Vertex AI Imagen: {output_path}")
                            return result
                    else:
                        logger.warning("Vertex AI Imagen not initialized, falling back")
                        
                except Exception as vertex_error:
                    logger.warning(f"Vertex AI Imagen failed: {vertex_error}")
            
            # Try Google AI Studio Imagen (future support)
            try:
                import google.generativeai as genai
                
                # This is placeholder for when Google AI Studio supports Imagen
                model = genai.GenerativeModel('imagen-3')
                
                logger.info(f"üé® Generating image with Imagen 3: {prompt[:50]}...")
                
                # This will fail for now but is ready for future support
                response = model.generate_images(
                    prompt=prompt,
                    number_of_images=1,
                    aspect_ratio="16:9",
                    safety_filter_level="block_some",
                    person_generation="allow_adult"
                )
                
                if response.images:
                    image = response.images[0]
                    image.save(output_path)
                    logger.info(f"‚úÖ Successfully generated image with Imagen 3: {output_path}")
                    return output_path
                    
            except Exception as imagen_error:
                logger.debug(f"Google AI Studio Imagen not available: {imagen_error}")
            
            # Fallback: Create a sophisticated AI-style placeholder
            success = self._create_sophisticated_placeholder(prompt, output_path)
            
            if success:
                logger.info(f"üì∏ Created enhanced visual placeholder: {output_path}")
                return output_path
            else:
                return None
                
        except Exception as e:
            logger.error(f"Image generation failed: {e}")
            return None

    def _create_sophisticated_placeholder(self, prompt: str, output_path: str) -> bool:
        """Create a sophisticated placeholder image with AI-style design"""
        try:
            from PIL import Image, ImageDraw, ImageFont, ImageFilter
            import random
            import math
            
            # Create high-resolution image
            width, height = 1920, 1080
            
            # Extract key elements from prompt for visual generation
            prompt_lower = prompt.lower()
            
            # Determine visual style based on prompt content
            if any(word in prompt_lower for word in ["b2", "bomber", "aircraft", "military", "stealth"]):
                # Military/aircraft theme
                colors = [(45, 45, 45), (70, 70, 70), (100, 100, 100), (30, 30, 50)]
                style = "military"
            elif any(word in prompt_lower for word in ["iran", "middle east", "desert"]):
                # Desert/Middle East theme
                colors = [(194, 154, 108), (166, 127, 89), (139, 90, 43), (205, 133, 63)]
                style = "desert"
            elif any(word in prompt_lower for word in ["nature", "forest", "green", "landscape"]):
                colors = [(34, 139, 34), (0, 100, 0), (144, 238, 144), (46, 125, 50)]
                style = "nature"
            elif any(word in prompt_lower for word in ["ocean", "water", "blue", "sky"]):
                colors = [(30, 144, 255), (0, 191, 255), (135, 206, 235), (70, 130, 180)]
                style = "ocean"
            elif any(word in prompt_lower for word in ["sunset", "warm", "orange", "fire"]):
                colors = [(255, 140, 0), (255, 69, 0), (255, 165, 0), (255, 99, 71)]
                style = "sunset"
            elif any(word in prompt_lower for word in ["tech", "digital", "ai", "modern"]):
                colors = [(75, 0, 130), (138, 43, 226), (147, 112, 219), (123, 104, 238)]
                style = "tech"
            else:
                colors = [(100, 149, 237), (72, 61, 139), (106, 90, 205), (123, 104, 238)]
                style = "default"
            
            # Create gradient background
            img = Image.new('RGB', (width, height), color=colors[0])
            draw = ImageDraw.Draw(img)
            
            # Create gradient effect
            for i in range(height):
                ratio = i / height
                r = int(colors[0][0] * (1 - ratio) + colors[1][0] * ratio)
                g = int(colors[0][1] * (1 - ratio) + colors[1][1] * ratio)
                b = int(colors[0][2] * (1 - ratio) + colors[1][2] * ratio)
                draw.line([(0, i), (width, i)], fill=(r, g, b))
            
            # Create style-specific visual elements with enhanced details
            if style == "military":
                # Create more detailed B2 bomber silhouette
                # Main body (triangular shape like B2)
                b2_body = [
                    (width//2, height//2 - 100),  # Nose
                    (width//2 - 350, height//2 + 50),  # Left wing tip
                    (width//2 - 250, height//2 + 40),  # Left wing indent
                    (width//2 - 150, height//2 + 60),  # Left engine
                    (width//2 - 50, height//2 + 50),   # Left inner
                    (width//2, height//2 + 70),        # Center rear
                    (width//2 + 50, height//2 + 50),   # Right inner
                    (width//2 + 150, height//2 + 60),  # Right engine
                    (width//2 + 250, height//2 + 40),  # Right wing indent
                    (width//2 + 350, height//2 + 50),  # Right wing tip
                ]
                
                # Draw shadow first
                shadow_points = [(x + 20, y + 20) for x, y in b2_body]
                draw.polygon(shadow_points, fill=(10, 10, 10, 150))
                
                # Draw main body
                draw.polygon(b2_body, fill=(35, 35, 40))
                
                # Add engine intakes
                for offset in [-150, -50, 50, 150]:
                    draw.ellipse([
                        width//2 + offset - 15, height//2 + 40,
                        width//2 + offset + 15, height//2 + 70
                    ], fill=(20, 20, 25))
                
                # Add cockpit window
                draw.ellipse([
                    width//2 - 20, height//2 - 80,
                    width//2 + 20, height//2 - 40
                ], fill=(60, 80, 100))
                
                # Add clouds/atmosphere
                for _ in range(5):
                    cloud_x = random.randint(0, width)
                    cloud_y = random.randint(0, height//2)
                    cloud_color = (200, 200, 200, 50)
                    for i in range(3):
                        draw.ellipse([
                            cloud_x + i*30, cloud_y,
                            cloud_x + i*30 + 80, cloud_y + 40
                        ], fill=cloud_color[:3])
                
            elif style == "desert":
                # Enhanced desert landscape
                # Sky gradient
                for i in range(height//2):
                    sky_ratio = i / (height//2)
                    r = int(135 * (1 - sky_ratio) + 255 * sky_ratio)
                    g = int(206 * (1 - sky_ratio) + 220 * sky_ratio)
                    b = int(235 * (1 - sky_ratio) + 170 * sky_ratio)
                    draw.line([(0, i), (width, i)], fill=(r, g, b))
                
                # Multiple layers of dunes with better shading
                dune_layers = [
                    {"y_base": 0.5, "amplitude": 60, "color": (205, 173, 125)},
                    {"y_base": 0.6, "amplitude": 50, "color": (194, 154, 108)},
                    {"y_base": 0.7, "amplitude": 40, "color": (183, 135, 91)},
                    {"y_base": 0.85, "amplitude": 30, "color": (166, 117, 74)},
                ]
                
                for layer_idx, layer in enumerate(dune_layers):
                    points = []
                    y_base = height * layer["y_base"]
                    
                    for x in range(-50, width + 50, 10):
                        # Create more natural dune shapes
                        y = y_base + math.sin(x * 0.005 + layer_idx) * layer["amplitude"]
                        y += math.sin(x * 0.01 + layer_idx * 2) * layer["amplitude"] * 0.3
                        points.append((x, y))
                    
                    points.append((width + 50, height))
                    points.append((-50, height))
                    draw.polygon(points, fill=layer["color"])
                    
                    # Add highlights on dunes
                    for x in range(0, width, 100):
                        if random.random() > 0.7:
                            highlight_y = y_base + math.sin(x * 0.005 + layer_idx) * layer["amplitude"] - 10
                            highlight_color = tuple(min(255, c + 30) for c in layer["color"])
                            draw.ellipse([x-20, highlight_y-5, x+20, highlight_y+5], fill=highlight_color)
                
                # Add sun
                sun_x, sun_y = width - 200, 150
                # Sun glow
                for i in range(50, 0, -5):
                    alpha = int(255 * (1 - i/50))
                    glow_color = (255, 200, 100)
                    draw.ellipse([sun_x-i, sun_y-i, sun_x+i, sun_y+i], fill=glow_color)
                # Sun core
                draw.ellipse([sun_x-30, sun_y-30, sun_x+30, sun_y+30], fill=(255, 255, 200))
                
            elif style == "tech":
                # Enhanced tech/circuit pattern
                # Create grid background
                grid_color = tuple(c + 20 for c in colors[0])
                for x in range(0, width, 50):
                    draw.line([(x, 0), (x, height)], fill=grid_color, width=1)
                for y in range(0, height, 50):
                    draw.line([(0, y), (width, y)], fill=grid_color, width=1)
                
                # Create circuit board pattern
                nodes = []
                for _ in range(15):
                    node = {
                        'x': random.randint(100, width - 100),
                        'y': random.randint(100, height - 100),
                        'size': random.randint(10, 25),
                        'color': random.choice(colors[1:])
                    }
                    nodes.append(node)
                
                # Connect nodes with circuit traces
                for i, node in enumerate(nodes):
                    # Draw traces to nearby nodes
                    for j, other in enumerate(nodes[i+1:], i+1):
                        if math.sqrt((node['x']-other['x'])**2 + (node['y']-other['y'])**2) < 300:
                            # Draw circuit trace (Manhattan routing)
                            mid_y = (node['y'] + other['y']) // 2
                            # Horizontal then vertical
                            draw.line([(node['x'], node['y']), (node['x'], mid_y)], 
                                    fill=node['color'], width=3)
                            draw.line([(node['x'], mid_y), (other['x'], mid_y)], 
                                    fill=node['color'], width=3)
                            draw.line([(other['x'], mid_y), (other['x'], other['y'])], 
                                    fill=node['color'], width=3)
                
                # Draw nodes on top
                for node in nodes:
                    # Outer ring
                    draw.ellipse([
                        node['x']-node['size'], node['y']-node['size'],
                        node['x']+node['size'], node['y']+node['size']
                    ], fill=node['color'])
                    # Inner core
                    inner_size = node['size'] // 2
                    draw.ellipse([
                        node['x']-inner_size, node['y']-inner_size,
                        node['x']+inner_size, node['y']+inner_size
                    ], fill=tuple(min(255, c + 50) for c in node['color']))
                
                # Add data flow visualization
                for _ in range(20):
                    x = random.randint(0, width)
                    y = random.randint(0, height)
                    size = random.randint(2, 5)
                    flow_color = random.choice([(100, 255, 100), (100, 100, 255), (255, 100, 100)])
                    draw.ellipse([x-size, y-size, x+size, y+size], fill=flow_color)
                    
            else:
                # Enhanced abstract/artistic style
                # Create flowing shapes
                for i in range(12):
                    # Create bezier-like curves
                    points = []
                    start_x = random.randint(0, width)
                    start_y = random.randint(0, height)
                    
                    for j in range(5):
                        x = start_x + j * 100 + random.randint(-50, 50)
                        y = start_y + math.sin(j * 0.5) * 100 + random.randint(-50, 50)
                        points.append((x, y))
                    
                    # Draw smooth curve through points
                    if len(points) > 2:
                        shape_color = random.choice(colors[1:])
                        for k in range(len(points) - 1):
                            draw.line([points[k], points[k+1]], fill=shape_color, width=5)
                        
                        # Add glowing nodes at points
                        for point in points:
                            for radius in range(20, 0, -4):
                                alpha = int(255 * (1 - radius/20))
                                glow = tuple(min(255, c + alpha//2) for c in shape_color)
                                draw.ellipse([
                                    point[0]-radius, point[1]-radius,
                                    point[0]+radius, point[1]+radius
                                ], fill=glow)
            
            # Apply artistic filters for more realistic look
            if style in ["military", "tech"]:
                # Add slight blur for depth
                img = img.filter(ImageFilter.GaussianBlur(radius=1))
            elif style == "desert":
                # Add warmth filter
                img = img.filter(ImageFilter.EDGE_ENHANCE_MORE)
            
            # Add subtle noise for realism
            noise_layer = Image.new('RGB', (width, height))
            noise_draw = ImageDraw.Draw(noise_layer)
            for _ in range(5000):
                x = random.randint(0, width)
                y = random.randint(0, height)
                brightness = random.randint(-20, 20)
                try:
                    pixel_color = img.getpixel((x, y))
                    color = tuple(max(0, min(255, c + brightness)) for c in pixel_color)
                except:
                    color = (128, 128, 128)  # Default gray if pixel access fails
                noise_draw.point((x, y), fill=color)
            
            # Blend noise layer
            img = Image.blend(img, noise_layer, 0.05)
            
            # Save the image
            img.save(output_path, "JPEG", quality=90)
            
            return True
            
        except Exception as e:
            logger.error(f"Sophisticated placeholder creation failed: {e}")
            return False

    def _create_placeholder_image(self, prompt: str, output_dir: str, filename: str) -> Optional[str]:
        """Create a simple placeholder image with prompt text"""
        try:
            from PIL import Image, ImageDraw, ImageFont
            
            output_path = os.path.join(output_dir, f"{filename}.jpg")
            
            # Create image
            width, height = 1280, 720
            img = Image.new('RGB', (width, height), color='lightblue')
            draw = ImageDraw.Draw(img)
            
            # Add text
            try:
                font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 40)
            except:
                font = ImageFont.load_default()
            
            # Wrap text
            words = prompt.split()
            lines = []
            current_line = []
            
            for word in words:
                test_line = ' '.join(current_line + [word])
                bbox = draw.textbbox((0, 0), test_line, font=font)
                if bbox[2] - bbox[0] <= width - 40:
                    current_line.append(word)
                else:
                    if current_line:
                        lines.append(' '.join(current_line))
                        current_line = [word]
                    else:
                        lines.append(word)
            
            if current_line:
                lines.append(' '.join(current_line))
            
            # Draw text lines
            y_offset = (height - len(lines) * 50) // 2
            for line in lines[:6]:  # Max 6 lines
                bbox = draw.textbbox((0, 0), line, font=font)
                text_width = bbox[2] - bbox[0]
                x_offset = (width - text_width) // 2
                draw.text((x_offset, y_offset), line, fill='black', font=font)
                y_offset += 50
            
            img.save(output_path, "JPEG", quality=85)
            return output_path
            
        except Exception as e:
            logger.error(f"Placeholder image creation failed: {e}")
            return None

    def _create_video_from_images(self, image_paths: List[str], duration: float, clip_id: str, prompt_data: Dict) -> str:
        """Create a video from a sequence of images with smooth transitions"""
        try:
            import subprocess
            
            output_path = os.path.join(self.clips_dir, f"image_based_clip_{clip_id}.mp4")
            
            if not image_paths:
                logger.error("No images provided for video creation")
                return self._create_descriptive_fallback_clip(prompt_data, duration, clip_id, None)
            
            # Calculate timing
            time_per_image = duration / len(image_paths)
            
            logger.info(f"üé¨ Creating video from {len(image_paths)} images ({time_per_image:.2f}s per image)")
            
            # Create input file list for ffmpeg
            input_list_path = os.path.join(os.path.dirname(output_path), f"input_list_{clip_id}.txt")
            
            with open(input_list_path, 'w') as f:
                for img_path in image_paths:
                    f.write(f"file '{img_path}'\n")
                    f.write(f"duration {time_per_image}\n")
                # Repeat last image to ensure proper duration
                if image_paths:
                    f.write(f"file '{image_paths[-1]}'\n")
            
            # Create video with smooth transitions
            cmd = [
                'ffmpeg', '-y',
                '-f', 'concat',
                '-safe', '0',
                '-i', input_list_path,
                '-vf', f'scale=1280:720,fps=30,fade=in:0:15,fade=out:{int(duration*30-15)}:15',
                '-c:v', 'libx264',
                '-preset', 'medium',
                '-crf', '23',
                '-pix_fmt', 'yuv420p',
                '-movflags', '+faststart',
                output_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and os.path.exists(output_path):
                file_size = os.path.getsize(output_path) / (1024 * 1024)
                logger.info(f"‚úÖ Image-based video created: {output_path} ({file_size:.1f}MB)")
                
                # Clean up temporary files
                try:
                    os.remove(input_list_path)
                except:
                    pass
                
                return output_path
            else:
                logger.error(f"FFmpeg failed: {result.stderr}")
                raise Exception(f"Video creation failed: {result.stderr}")
                
        except Exception as e:
            logger.error(f"Video from images creation failed: {e}")
            # Fallback to descriptive
            return self._create_descriptive_fallback_clip(prompt_data, duration, clip_id, None)

    def _create_descriptive_fallback_clip(self, prompt_data: Dict, duration: float, clip_id: str, config: Optional[GeneratedVideoConfig]) -> str:
        """Create enhanced fallback clip with detailed description of what would be seen"""
        output_path = os.path.join(self.clips_dir, f"fallback_{clip_id}.mp4")
        
        # Get the scene description and VEO2 prompt
        description = prompt_data.get('description', 'Video scene')
        veo2_prompt = prompt_data.get('veo2_prompt', 'Video content')
        
        # Create descriptive text that explains what would be seen
        descriptive_text = f"SCENE: {description}\n\nWOULD SHOW:\n{veo2_prompt[:100]}..."
        
        # Create background with gradient based on content
        if any(word in description.lower() for word in ['baby', 'child', 'family']):
            bg_color = (255, 182, 193)  # Light pink
        elif any(word in description.lower() for word in ['nature', 'outdoor', 'landscape']):
            bg_color = (173, 216, 230)  # Light blue
        elif any(word in description.lower() for word in ['news', 'breaking', 'viral']):
            bg_color = (255, 99, 71)    # Tomato red
        else:
            bg_color = (100, 150, 200)  # Default blue
        
        try:
            background = ColorClip(size=(1080, 1920), color=bg_color, duration=duration)
            
            # Create descriptive text overlay
            text_img = self._create_descriptive_text_with_pil(descriptive_text, (1080, 600))
            text_clip = ImageClip(text_img, duration=duration).set_position('center')
            
            final_clip = CompositeVideoClip([background, text_clip])
            
            # Write the video file with fps specified
            final_clip.write_videofile(output_path, codec='libx264', audio=False, fps=24, verbose=False, logger=None)
            
            # Clean up
            final_clip.close()
            background.close()
            text_clip.close()
            
            return output_path
            
        except Exception as e:
            logger.error(f"Error creating descriptive fallback: {e}")
            # Create simple fallback
            return self._create_simple_fallback_clip(prompt_data, duration, clip_id)

    def _create_descriptive_text_with_pil(self, text: str, size: Tuple[int, int]) -> np.ndarray:
        """Create descriptive text image using PIL with word wrapping"""
        try:
            from PIL import Image, ImageDraw, ImageFont
            
            # Create image
            img = Image.new('RGBA', size, (0, 0, 0, 0))
            draw = ImageDraw.Draw(img)
            
            # Try to use a system font
            try:
                font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 36)
                title_font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 48)
            except:
                try:
                    font = ImageFont.truetype("arial.ttf", 36)
                    title_font = ImageFont.truetype("arial.ttf", 48)
                except:
                    font = ImageFont.load_default()
                    title_font = ImageFont.load_default()
            
            # Split text into lines and wrap
            lines = text.split('\n')
            wrapped_lines = []
            
            for line in lines:
                if len(line) > 40:  # Wrap long lines
                    words = line.split(' ')
                    current_line = ""
                    for word in words:
                        if len(current_line + word) < 40:
                            current_line += word + " "
                        else:
                            if current_line:
                                wrapped_lines.append(current_line.strip())
                            current_line = word + " "
                    if current_line:
                        wrapped_lines.append(current_line.strip())
                else:
                    wrapped_lines.append(line)
            
            # Draw text with different fonts for title and content
            y_offset = 50
            for i, line in enumerate(wrapped_lines):
                if line.startswith("SCENE:"):
                    current_font = title_font
                    color = (255, 255, 255, 255)  # White
                elif line.startswith("WOULD SHOW:"):
                    current_font = title_font
                    color = (255, 255, 0, 255)   # Yellow
                else:
                    current_font = font
                    color = (255, 255, 255, 255)  # White
                
                # Center text
                bbox = draw.textbbox((0, 0), line, font=current_font)
                text_width = bbox[2] - bbox[0]
                x_offset = (size[0] - text_width) // 2
                
                # Draw text with shadow
                draw.text((x_offset + 2, y_offset + 2), line, font=current_font, fill=(0, 0, 0, 128))
                draw.text((x_offset, y_offset), line, font=current_font, fill=color)
                
                y_offset += 60
            
            # Convert to numpy array
            return np.array(img)
            
        except Exception as e:
            logger.error(f"PIL text creation failed: {e}")
            # Return simple colored rectangle
            return np.full((size[1], size[0], 4), [255, 255, 255, 255], dtype=np.uint8)

    def _create_simple_fallback_clip(self, prompt_data: Dict, duration: float, clip_id: str) -> str:
        """Create simple fallback clip as last resort"""
        output_path = os.path.join(self.clips_dir, f"simple_{clip_id}.mp4")
        
        try:
            background = ColorClip(size=(1080, 1920), color=(100, 150, 200), duration=duration)
            background.write_videofile(output_path, codec='libx264', audio=False, fps=24, verbose=False, logger=None)
            background.close()
            return output_path
        except Exception as e:
            logger.error(f"Even simple fallback failed: {e}")
            return output_path
    
    def _create_fallback_clip(self, prompt_data: Dict, duration: float, clip_id: str) -> str:
        """Create fallback clip if Veo-2 generation fails"""
        output_path = os.path.join(self.clips_dir, f"fallback_{clip_id}.mp4")
        
        # Create better fallback content
        background = ColorClip(size=(1080, 1920), color=(100, 150, 200), duration=duration)
        
        try:
            text_img = self.veo_client._create_text_with_pil(
                f"Scene: {prompt_data['description'][:40]}...", 
                (1080, 300)
            )
            text_clip = ImageClip(text_img, duration=duration).set_position('center')
            final_clip = CompositeVideoClip([background, text_clip])
        except:
            final_clip = background
        
        final_clip.write_videofile(output_path, codec='libx264', audio=False, verbose=False, logger=None)
        return output_path
    
    def _compose_video_with_veo_clips(self, veo_clips: List[Dict], audio_path: str, 
                                      config: GeneratedVideoConfig, video_id: str) -> str:
        """Compose final video using real Veo-2 clips with proper sequential playback and frame continuity"""
        try:
            # Load audio to get exact duration
            audio_clip = AudioFileClip(audio_path)
            audio_duration = audio_clip.duration
            
            logger.info(f"Audio duration: {audio_duration:.2f}s, Target: {config.duration_seconds}s")
            logger.info(f"Frame continuity mode: {config.frame_continuity}")
            
            # Load and prepare video clips - PLAY EACH CLIP ONCE
            video_clips = []
            total_video_duration = 0
            
            for i, clip_info in enumerate(veo_clips):
                try:
                    # Load Veo-2 clip
                    video_clip = VideoFileClip(clip_info['clip_path'])
                    
                    # Handle frame continuity - remove duplicate frames at boundaries
                    if config.frame_continuity and i > 0:
                        # Remove first frame of current clip to avoid duplicate with last frame of previous
                        # Get framerate and calculate frame duration
                        fps = video_clip.fps
                        frame_duration = 1.0 / fps
                        
                        # Trim first frame by starting slightly later
                        video_clip = video_clip.subclip(frame_duration)
                        
                        logger.info(f"Frame continuity: Removed first frame ({frame_duration:.3f}s) from clip {i+1}")
                    
                    # Just use the clip as-is - NO LOOPING
                    # Each clip will play once in sequence
                    video_clips.append(video_clip)
                    total_video_duration += video_clip.duration
                    
                    logger.info(f"Loaded clip {i+1}: {video_clip.duration:.2f}s ({clip_info['clip_path']})")
                    
                except Exception as e:
                    logger.warning(f"Error loading clip {clip_info['clip_path']}: {e}")
                    # Create emergency fallback with appropriate duration
                    expected_clip_duration = audio_duration / len(veo_clips)
                    fallback = ColorClip(size=(1080, 1920), color=(50, 50, 50), 
                                       duration=expected_clip_duration)
                    video_clips.append(fallback)
                    total_video_duration += expected_clip_duration
            
            # Apply frame continuity transitions if enabled
            if config.frame_continuity and len(video_clips) > 1:
                continuity_details = getattr(config, '_continuity_details', {})
                transition_strategy = continuity_details.get('transition_strategy', {})
                
                if transition_strategy.get('frame_blend_duration', 0) > 0:
                    # Apply smooth transitions between clips
                    blend_duration = transition_strategy['frame_blend_duration']
                    logger.info(f"Applying frame blend transitions ({blend_duration}s) for continuity")
                    
                    # Create transitions between clips
                    transitioned_clips = []
                    for i, clip in enumerate(video_clips):
                        if i == 0:
                            transitioned_clips.append(clip)
                        else:
                            # Add crossfade transition
                            transition = CompositeVideoClip([
                                transitioned_clips[-1].fadeout(blend_duration),
                                clip.fadein(blend_duration)
                            ])
                            transitioned_clips[-1] = transitioned_clips[-1].set_end(
                                transitioned_clips[-1].duration - blend_duration
                            )
                            transitioned_clips.append(clip)
                    
                    video_clips = transitioned_clips
            
            # Concatenate all video clips in sequence - each plays once
            if video_clips:
                final_video = concatenate_videoclips(video_clips, method="compose")
                
                logger.info(f"Concatenated video duration: {final_video.duration:.2f}s")
                
                # üé≠ ENHANCED AI AGENT ORCHESTRATION FIX
                # CRITICAL: Video duration takes precedence - don't trim generated clips!
                video_duration = final_video.duration
                target_duration = video_duration  # Use actual video duration as target
                
                logger.info(f"üéØ Enhanced Duration Orchestration:")
                logger.info(f"   Video clips total: {video_duration:.2f}s")
                logger.info(f"   Audio duration: {audio_duration:.2f}s") 
                logger.info(f"   Target duration: {target_duration:.2f}s (video takes precedence)")
                logger.info(f"   Frame continuity: {'ENABLED' if config.frame_continuity else 'DISABLED'}")
                
                # VIDEO AGENT: Keep all generated video clips (no trimming)
                # Only adjust if video is significantly different from config target
                config_target = config.duration_seconds
                if abs(video_duration - config_target) > config_target * 0.5:
                    logger.warning(f"Video duration {video_duration:.1f}s differs significantly from config {config_target}s")
                
                # SOUNDMAN AGENT: Adjust audio to match video perfectly
                if abs(audio_duration - video_duration) > 1.0:  # More than 1 second difference
                    if audio_duration > video_duration:
                        # Audio is longer - extend video to match audio
                        logger.info(f"üé¨ Audio longer ({audio_duration:.2f}s) than video ({video_duration:.2f}s)")
                        
                        # Calculate how much to extend
                        extension_needed = audio_duration - video_duration
                        
                        if extension_needed <= 10.0:  # Reasonable extension
                            # Extend video by looping the last clip or creating static frames
                            try:
                                # Get the last video clip and extend it
                                last_clip = video_clips[-1] if video_clips else None
                                if last_clip and hasattr(last_clip, 'duration'):
                                    # Extend the last clip
                                    extended_last_clip = last_clip.loop(duration=last_clip.duration + extension_needed)
                                    video_clips[-1] = extended_last_clip
                                    
                                    # Recreate the final video with extended duration
                                    final_video = concatenate_videoclips(video_clips, method="compose")
                                    video_duration = final_video.duration
                                    logger.info(f"üé¨ Extended video to match audio: {video_duration:.2f}s")
                                else:
                                    # Fallback: trim audio to match video
                                    audio_clip = audio_clip.subclip(0, video_duration)
                                    logger.info(f"üé§ Trimmed audio to match video: {video_duration:.2f}s")
                            except Exception as e:
                                logger.warning(f"Failed to extend video: {e}")
                                # Fallback: trim audio to match video
                                audio_clip = audio_clip.subclip(0, video_duration)
                                logger.info(f"üé§ Trimmed audio to match video: {video_duration:.2f}s")
                        else:
                            # Too much extension needed - trim audio instead
                            audio_clip = audio_clip.subclip(0, video_duration)
                            logger.info(f"üé§ Audio too long - trimmed to match video: {video_duration:.2f}s")
                    
                    elif audio_duration < video_duration:
                        # Audio is shorter - extend audio to match video
                        ratio = video_duration / audio_duration
                        
                        if ratio <= 2.0:
                            # Slow down audio slightly for natural extension using fl_time
                            speed_factor = audio_duration / video_duration
                            audio_clip = audio_clip.fl_time(lambda t: t * speed_factor, apply_to=['mask', 'audio'])
                            # Ensure duration is properly set after transformation
                            audio_clip = audio_clip.set_duration(video_duration)
                            logger.info(f"üé§ ORCHESTRATED: Slowed audio naturally to {video_duration:.2f}s")
                        else:
                            # Loop with fade transitions for longer extensions
                            loops_needed = int(video_duration / audio_duration) + 1
                            audio_loops = []
                            for i in range(loops_needed):
                                loop_audio = audio_clip
                                if i > 0:
                                    # Add fade in for seamless looping
                                    loop_audio = loop_audio.fx(fadein, 0.5)
                                if i < loops_needed - 1:
                                    # Add fade out for seamless looping
                                    loop_audio = loop_audio.fx(fadeout, 0.5)
                                audio_loops.append(loop_audio)
                            
                            extended_audio = concatenate_audioclips(audio_loops)
                            audio_clip = extended_audio.subclip(0, video_duration)
                            # Ensure duration is properly set
                            audio_clip = audio_clip.set_duration(video_duration)
                            logger.info(f"üé§ ORCHESTRATED: Looped audio with fades to {video_duration:.2f}s")
                else:
                    # Even if durations are close, ensure audio duration is properly set
                    audio_clip = audio_clip.set_duration(video_duration)
                    logger.info(f"üé§ ORCHESTRATED: Audio duration acceptable ({audio_duration:.2f}s vs {video_duration:.2f}s)")
                
                # Update target duration to actual final duration
                final_duration = max(video_duration, audio_clip.duration)
                
                # DIRECTOR AGENT: Log script-video alignment
                if hasattr(config, 'frame_continuity') and config.frame_continuity:
                    logger.info("üé≠ ORCHESTRATED: Frame continuity enabled - seamless visual flow")
                    continuity_details = getattr(config, '_continuity_details', {})
                    if continuity_details:
                        logger.info(f"   Continuity type: {continuity_details.get('transition_strategy', {}).get('type', 'unknown')}")
                        logger.info(f"   Clip count: {len(video_clips)} clips")
                
                logger.info(f"‚úÖ AGENT ORCHESTRATION COMPLETE: {final_duration:.2f}s synchronized video")
                
                # Add text overlays to the video
                final_video_with_overlays = self._add_text_overlays(final_video, config, final_duration)
                
                # Add audio to video - ensure both have proper durations
                final_video_with_audio = final_video_with_overlays.set_audio(audio_clip)
                
                # Ensure final video has proper duration
                final_video_with_audio = final_video_with_audio.set_duration(final_duration)
                
                # Output path
                output_path = os.path.join(self.output_dir, f"viral_video_{video_id}.mp4")
                
                # Render with hardware acceleration if available
                final_video_with_audio.write_videofile(
                    output_path,
                    codec='libx264',
                    audio_codec='aac',
                    temp_audiofile='temp-audio.m4a',
                    remove_temp=True,
                    verbose=False,
                    logger=None
                )
                
                # Clean up
                final_video_with_audio.close()
                audio_clip.close()
                
                logger.info(f"Final video duration: {audio_duration:.2f}s matches audio: {audio_duration:.2f}s")
                return output_path
                
            else:
                raise Exception("No video clips were successfully loaded")
                
        except Exception as e:
            logger.error(f"Error composing video: {e}")
            raise

    def _generate_veo2_prompts(self, config: GeneratedVideoConfig, script: str) -> List[Dict]:
        """Generate VEO2 prompts from the script with frame continuity support"""
        try:
            logger.info(f"Generating VEO2 prompts (continuity mode: {config.frame_continuity})")
            
            # Get continuity details if available
            continuity_details = getattr(config, '_continuity_details', None)
            transition_strategy = continuity_details.get('transition_strategy') if continuity_details else None
            
            # Determine number of clips based on continuity mode
            if config.frame_continuity and continuity_details:
                num_clips = continuity_details['recommended_clip_count']
            else:
                # Calculate based on duration - aim for 5-10 second clips
                num_clips = max(3, min(config.duration_seconds // 7, 8))
            
            duration_per_clip = config.duration_seconds / num_clips
            
            # Create prompt for VEO2 generation with continuity awareness
            continuity_instructions = ""
            if config.frame_continuity and transition_strategy:
                continuity_instructions = f"""
                IMPORTANT: This video uses frame continuity. {transition_strategy['config']['instructions']}
                Transition type: {transition_strategy['type']}
                
                For seamless continuity:
                - End each clip in a way that naturally leads to the next
                - Maintain consistent lighting and color grading
                - Keep key subjects in similar positions at clip boundaries
                - Create motion or visual elements that can flow between clips
                """
            
            prompt = f"""
            Create {num_clips} detailed video scene descriptions for VEO2 generation.
            
            Video Details:
            - Topic: {config.topic}
            - Style: {config.style} 
            - Duration: {config.duration_seconds} seconds total ({duration_per_clip:.1f}s per clip)
            - Platform: {config.target_platform.value}
            - Visual Style: {config.visual_style}
            - Narrative: {config.narrative.value}
            - Feeling: {config.feeling.value}
            
            {continuity_instructions}
            
            Script:
            {script}
            
            For each scene, provide:
            1. A detailed visual description (what the viewer sees)
            2. A VEO2 prompt that captures the essence
            3. Camera movement and framing
            4. Key visual elements and colors
            {5 if config.frame_continuity else ''}
            
            {"5. Continuity notes: How this scene connects to the next" if config.frame_continuity else ""}
            
            Return ONLY a valid JSON array with NO additional text or formatting:
            [
                {{
                    "scene_index": 0,
                    "description": "detailed scene description",
                    "veo2_prompt": "VEO2 generation prompt",
                    "camera_movement": "static/pan/zoom/etc",
                    "key_elements": ["element1", "element2"],
                    "duration": {duration_per_clip}{"," if config.frame_continuity else ""}
                    {"\"continuity_notes\": \"how this connects to next scene\"" if config.frame_continuity else ""}
                }},
                ...
            ]
            
            IMPORTANT: Return ONLY the JSON array, no markdown formatting, no additional text.
            Make each scene visually engaging and appropriate for {config.target_platform.value}.
            """
            
            response = self.script_model.generate_content(prompt)
            
            # Extract JSON from response
            veo_prompts = self._extract_json_safely(response.text)
            
            if not veo_prompts or not isinstance(veo_prompts, list):
                logger.warning("Failed to parse VEO2 prompts, using fallback")
                veo_prompts = self._generate_fallback_veo2_prompts(config, num_clips, duration_per_clip)
            
            # Enhance prompts with continuity if needed
            if config.frame_continuity:
                veo_prompts = self._enhance_prompts_for_continuity(veo_prompts, transition_strategy)
            
            # Review and refine prompts
            veo_prompts = self._review_and_refine_veo2_prompts(veo_prompts, config)
            
            logger.info(f"Generated {len(veo_prompts)} VEO2 prompts")
                return veo_prompts
                
        except Exception as e:
            logger.error(f"Failed to generate VEO2 prompts: {e}")
            return self._generate_fallback_veo2_prompts(config, 5, config.duration_seconds / 5)
    
    def _enhance_prompts_for_continuity(self, prompts: List[Dict], 
                                      transition_strategy: Optional[Dict]) -> List[Dict]:
        """Enhance prompts with specific continuity instructions"""
        if not transition_strategy:
            return prompts
            
        enhanced_prompts = []
        
        for i, prompt in enumerate(prompts):
            enhanced = prompt.copy()
            
            # Add continuity instructions to VEO2 prompt
            if i < len(prompts) - 1:  # Not the last clip
                next_prompt = prompts[i + 1]
                
                # Add ending instruction
                continuity_suffix = self._get_continuity_suffix(
                    transition_strategy['type'], 
                    prompt, 
                    next_prompt
                )
                
                enhanced['veo2_prompt'] += f" {continuity_suffix}"
                
            if i > 0:  # Not the first clip
                prev_prompt = prompts[i - 1]
                
                # Add starting instruction
                continuity_prefix = self._get_continuity_prefix(
                    transition_strategy['type'],
                    prev_prompt,
                    prompt
                )
                
                enhanced['veo2_prompt'] = f"{continuity_prefix} {enhanced['veo2_prompt']}"
            
            enhanced['continuity_enabled'] = True
            enhanced['transition_type'] = transition_strategy['type']
            
            enhanced_prompts.append(enhanced)
            
        return enhanced_prompts
    
    def _get_continuity_suffix(self, transition_type: str, current: Dict, next: Dict) -> str:
        """Get continuity instruction for end of clip"""
        suffixes = {
            'smooth_motion': "End with rightward camera movement or subject motion",
            'object_tracking': "Keep main subject centered and facing forward at end",
            'environment_flow': "End with a view that suggests the next location",
            'narrative_continuity': "End mid-action or mid-sentence for continuity"
        }
        
        return suffixes.get(transition_type, "End naturally to flow into next scene")
    
    def _get_continuity_prefix(self, transition_type: str, prev: Dict, current: Dict) -> str:
        """Get continuity instruction for start of clip"""
        prefixes = {
            'smooth_motion': "Start with leftward motion continuing from previous",
            'object_tracking': "Begin with main subject in same position as previous end",
            'environment_flow': "Start in location suggested by previous scene",
            'narrative_continuity': "Begin continuing the action or thought from previous"
        }
        
        return prefixes.get(transition_type, "Start naturally from previous scene")
    
    def _generate_fallback_veo2_prompts(self, config: GeneratedVideoConfig, 
                                      num_clips: int, duration_per_clip: float) -> List[Dict]:
        """Generate fallback VEO2 prompts if AI generation fails"""
        prompts = []
        
        for i in range(num_clips):
            scene_type = self._get_scene_type(i, num_clips)
            
            base_prompt = self._generate_fallback_veo2_prompt(
                config.topic, scene_type, duration_per_clip
            )
            
            prompt_data = {
                "scene_index": i,
                "description": f"{scene_type} scene for {config.topic}",
                "veo2_prompt": base_prompt,
                "camera_movement": "dynamic",
                "key_elements": [config.topic, config.style],
                "duration": duration_per_clip
            }
            
            if config.frame_continuity:
                prompt_data["continuity_notes"] = f"Flows from scene {i} to {i+1}"
            
            prompts.append(prompt_data)
            
        return prompts
    
    def _generate_fallback_veo2_prompt(self, topic: str, scene_type: str, duration: float) -> str:
        """Generate a fallback Veo-2 prompt for any topic and scene type"""
        return f"""Create a REALISTIC video for: {topic}
{scene_type} scene with professional quality content.
Natural lighting, smooth camera work, engaging visuals.
Show {scene_type.lower()} content that advances the story.
Duration: {duration:.1f} seconds."""
    
    def _generate_baby_animal_prompt(self, scene_type: str, duration: float) -> str:
        """Generate baby + animal specific prompts"""
        prompts = {
            "Opening": """Create a REALISTIC amateur home video:
Adorable baby (8-12 months) sitting on soft blanket in cozy living room.
Small puppy and fluffy bunny nearby, calm family pets in safe environment.
Vertical 9:16 smartphone format, natural home lighting, authentic family moment.""",
            
            "Introduction": """Create a REALISTIC amateur family video:
Baby notices the gentle animals nearby, eyes lighting up with curiosity.
Small puppy approaches slowly, bunny sits peacefully in background.
Natural family video style, heartwarming introduction.""",
            
            "Main action": """Create a REALISTIC amateur family video:
Baby reaches out curiously toward small puppy who sniffs baby's hand gently.
Fluffy bunny hops closer, baby giggles softly with wonder and joy.
Natural interaction, vertical family video format.""",
            
            "Climax": """Create a REALISTIC amateur family video:
Baby and puppy playing gently while bunny watches nearby.
Pure joy and laughter as baby claps hands with delight.
Heartwarming peak moment of bonding.""",
            
            "Conclusion": """Create a REALISTIC amateur family video:
Baby sits happily surrounded by gentle puppy and bunny, all peaceful together.
Everyone content and relaxed, perfect family moment captured.
Vertical family video format, pure joy and bonding."""
        }
        
        return prompts.get(scene_type, prompts["Main action"])

    def _save_prompts_to_files(self, script: str, veo_prompts: List[Dict], video_id: str, config: GeneratedVideoConfig):
        """Save all prompts and scripts to text files"""
        # Save main script
        script_path = os.path.join(self.output_dir, f"script_{video_id}.txt")
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write("=== VIRAL VIDEO SCRIPT ===\n")
            f.write(f"Generated with: Gemini 2.5 Flash + Gemini 2.5 Pro\n")
            f.write(f"Video ID: {video_id}\n\n")
            f.write(script)
        
        # Save clean script for TTS - use actual config duration
        clean_script = self._clean_script_for_tts(script, config.duration_seconds)
        tts_script_path = os.path.join(self.output_dir, f"tts_script_{video_id}.txt")
        with open(tts_script_path, 'w', encoding='utf-8') as f:
            f.write("=== CLEAN SCRIPT FOR VOICEOVER ===\n")
            f.write(f"Video ID: {video_id}\n")
            f.write(f"Duration: {config.duration_seconds} seconds\n\n")
            f.write(clean_script)
        
        # Save Veo-2 prompts
        veo_prompts_path = os.path.join(self.output_dir, f"veo2_prompts_{video_id}.txt")
        with open(veo_prompts_path, 'w', encoding='utf-8') as f:
            f.write("=== VEO-2 VIDEO GENERATION PROMPTS ===\n")
            f.write(f"Video ID: {video_id}\n\n")
            for i, prompt in enumerate(veo_prompts):
                f.write(f"SCENE {prompt.get('scene_id', prompt.get('scene_index', i))}:\n")
                f.write(f"Description: {prompt['description']}\n")
                f.write(f"Duration: {prompt['duration']} seconds\n\n")
                f.write("VEO-2 PROMPT:\n")
                f.write(prompt['veo2_prompt'])
                f.write("\n" + "="*60 + "\n\n")
        
        logger.info(f"Saved prompts to: {script_path}, {tts_script_path}, {veo_prompts_path}")
            
    def _generate_voiceover(self, script: str, duration: int = 30, config: Dict = None) -> str:
        """Generate AI voice-over with emotion and feeling"""
        logger.info(f"Generating voice-over for {duration}s video...")
        
        if not config:
            config = {}
        
        # Extract narrative context and feeling for appropriate voice
        narrative_context = config.get('narrative', 'neutral')
        feeling_context = config.get('feeling', 'neutral')
        realistic_audio = config.get('realistic_audio', False)
        
        try:
            # Step 1: Generate natural TTS script
            clean_script = self._clean_script_for_tts(script, duration)
            
            # Step 2: Add emotional context
            emotional_script = self._add_emotional_context_to_tts(clean_script, narrative_context, feeling_context)
            
            # Step 3: Try Google Cloud TTS if realistic_audio is enabled
            if realistic_audio:
                try:
                    from .google_tts_client import GoogleTTSClient
                    
                    logger.info("üé§ Using Google Cloud TTS for natural voice...")
                    google_tts = GoogleTTSClient()
                    
                    audio_path = google_tts.generate_speech(
                        text=emotional_script,
                        feeling=feeling_context,
                        narrative=narrative_context,
                        duration_target=duration
                    )
                    
                    if audio_path and os.path.exists(audio_path):
                        # Move to output directory
                        final_path = os.path.join(self.output_dir, f"google_neural_voice_{uuid.uuid4()}.mp3")
                        os.rename(audio_path, final_path)
                        logger.info(f"‚úÖ Google Cloud TTS SUCCESS: Natural {feeling_context} voice generated")
                        return final_path
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Google Cloud TTS failed: {e}")
                    logger.info("üîÑ Falling back to basic gTTS...")
            
            # Step 4: Generate with enhanced emotion settings (gTTS fallback)
            try:
                tts_params = self._get_emotional_tts_settings(feeling_context)
                
                tts = gTTS(
                    text=emotional_script,
                    **tts_params
                )
                
                audio_path = os.path.join(self.output_dir, f"emotional_voiceover_{uuid.uuid4()}.mp3")
                tts.save(audio_path)
                
                logger.info(f"‚úÖ Basic TTS fallback generated: {audio_path}")
                return audio_path
            except Exception as gtts_error:
                logger.error(f"‚ùå Even basic gTTS failed: {gtts_error}")
                # Try one more time with simplest settings
                try:
                    simple_tts = gTTS(text=emotional_script, lang='en', slow=False)
                    audio_path = os.path.join(self.output_dir, f"fallback_voiceover_{uuid.uuid4()}.mp3")
                    simple_tts.save(audio_path)
                    logger.info(f"‚úÖ Simple TTS fallback generated: {audio_path}")
                    return audio_path
                except:
                    pass
        
        except Exception as e:
            logger.error(f"‚ùå All voice generation failed: {e}")
            # Return None to indicate failure
            return None
    
    def _clean_script_for_tts(self, script: str, target_duration: int) -> str:
        """Clean and optimize script for TTS generation with proper duration targeting"""
        
        # NO REMOVAL - Just extract and optimize for TTS
        # Keep all viral phrases but ensure variety in generation
        
        # Extract meaningful dialogue content
        dialogue_lines = []
        for line in script.split('\n'):
            if '**VOICEOVER:**' in line:
                content = line.replace('**VOICEOVER:**', '').strip()
                if content and len(content) > 10:  # Only meaningful content
                    dialogue_lines.append(content)
        
        if not dialogue_lines:
            # Fallback: extract any meaningful content
            sentences = script.split('.')
            dialogue_lines = [s.strip() for s in sentences if len(s.strip()) > 15]
        
        # Calculate optimal word count for target duration (2.5 words per second)
        target_words = int(target_duration * 2.5)
        
        # Join and trim to target length
        full_dialogue = ' '.join(dialogue_lines)
        words = full_dialogue.split()
                
                if len(words) > target_words:
            # Trim to target length but keep complete sentences
            trimmed_words = words[:target_words]
            # Try to end at a sentence boundary
            trimmed_text = ' '.join(trimmed_words)
            if '.' in trimmed_text:
                sentences = trimmed_text.split('.')
                if len(sentences) > 1:
                    # Keep complete sentences
                    trimmed_text = '. '.join(sentences[:-1]) + '.'
            final_script = trimmed_text
            else:
            final_script = full_dialogue
        
        # Ensure it ends properly
        if not final_script.endswith('.'):
            final_script += '.'
        
        logger.info(f"‚úÖ Optimized TTS script: {len(words)} words ‚Üí {len(final_script.split())} words")
        logger.info(f"üéØ Target duration: {target_duration}s, Target words: {target_words}")
        
        return final_script
    
    def _extract_dialogue_from_script(self, script: str) -> str:
        """Extract clean dialogue and narration from script"""
        import re
        
        # PRIMARY: Extract VOICEOVER dialogue (the main format in current scripts)
        voiceover_dialogue = []
        voiceover_matches = re.findall(r'\*\*VOICEOVER:\*\*\s*([^*\n]+)', script, re.IGNORECASE)
        
        for match in voiceover_matches:
            clean_line = match.strip()
            # Remove emotional direction brackets like **[Sharp, audible gasp]**
            clean_line = re.sub(r'\*\*\[.*?\]\*\*', '', clean_line)
            # Remove square bracket emotional directions like [Voice cracks with disbelief]
            clean_line = re.sub(r'\[.*?\]', '', clean_line)
            # Remove ALL parenthetical descriptions like "(Whispering, intrigued)"
            clean_line = re.sub(r'\([^)]*\)', '', clean_line)
            # Remove sound effect markers and technical content
            clean_line = re.sub(r'SFX:.*', '', clean_line)
            clean_line = re.sub(r'VISUAL:.*', '', clean_line)
            clean_line = re.sub(r'Epic.*music.*', '', clean_line, re.IGNORECASE)
            clean_line = re.sub(r'Music.*', '', clean_line, re.IGNORECASE)
            clean_line = re.sub(r'Sound.*', '', clean_line, re.IGNORECASE)
            clean_line = re.sub(r'Gentle.*chimes.*', '', clean_line, re.IGNORECASE)
            # Remove timing markers
            clean_line = re.sub(r'\d+:\d+', '', clean_line)
            clean_line = re.sub(r'\d+s\)', '', clean_line)
            
            # üé§ FIX: Remove viral phrases that shouldn't be spoken
            clean_line = re.sub(r'\bLIKE\b', 'like', clean_line, re.IGNORECASE)  # Convert to natural speech
            clean_line = re.sub(r'\bPOV:\s*', '', clean_line, re.IGNORECASE)  # Remove POV prefix
            clean_line = re.sub(r'\bY\'all\b', 'You all', clean_line, re.IGNORECASE)  # Make more natural
            clean_line = re.sub(r'\bWait,\s*what\?!', 'Wait, what?', clean_line, re.IGNORECASE)  # Clean punctuation
            
            # Clean up extra whitespace and punctuation
            clean_line = re.sub(r'\s+', ' ', clean_line).strip()
            clean_line = re.sub(r'^[:\-\s]+', '', clean_line)  # Remove leading colons/dashes
            clean_line = re.sub(r'[.]+$', '.', clean_line)  # Fix multiple periods
            
            if clean_line and len(clean_line) > 5:
                voiceover_dialogue.append(clean_line)
        
        if voiceover_dialogue:
            result = " ".join(voiceover_dialogue)
            logger.info(f"üé§ Extracted VOICEOVER dialogue: {result}")
            return result
        
        # SECONDARY: Extract NARRATOR dialogue (legacy format)
        narrator_dialogue = []
        narrator_matches = re.findall(r'\*\*NARRATOR[^:]*:\*\*\s*([^*\n]+)', script, re.IGNORECASE)
        
        for match in narrator_matches:
            clean_line = match.strip()
            clean_line = re.sub(r'\*\*\[.*?\]\*\*', '', clean_line)
            clean_line = re.sub(r'\[.*?\]', '', clean_line)
            clean_line = re.sub(r'\([^)]*\)', '', clean_line)
            clean_line = re.sub(r'SFX:.*', '', clean_line)
            clean_line = re.sub(r'VISUAL:.*', '', clean_line)
            clean_line = re.sub(r'\s+', ' ', clean_line).strip()
            
            if clean_line and len(clean_line) > 5:
                narrator_dialogue.append(clean_line)
        
        if narrator_dialogue:
            result = " ".join(narrator_dialogue)
            logger.info(f"üé§ Extracted NARRATOR dialogue: {result}")
            return result
        
        # TERTIARY: Extract VO (short voiceover) dialogue 
        vo_dialogue = []
        vo_matches = re.findall(r'\*\*VO[^:]*:\*\*\s*([^*\n]+)', script, re.IGNORECASE)
        
        for match in vo_matches:
            clean_line = match.strip()
            clean_line = re.sub(r'\*\*\[.*?\]\*\*', '', clean_line)
            clean_line = re.sub(r'\[.*?\]', '', clean_line)
            clean_line = re.sub(r'\([^)]*\)', '', clean_line)
            clean_line = re.sub(r'SFX:.*', '', clean_line)
            clean_line = re.sub(r'\s+', ' ', clean_line).strip()
            if clean_line and len(clean_line) > 3:
                vo_dialogue.append(clean_line)
        
        if vo_dialogue:
            result = " ".join(vo_dialogue)
            logger.info(f"üé§ Extracted VO dialogue: {result}")
            return result
            
        # FOURTH: Extract CHILD dialogue 
        child_dialogue = []
        child_matches = re.findall(r'\*\*CHILD[^:]*:\*\*\s*([^*\n]+)', script, re.IGNORECASE)
        
        for match in child_matches:
            clean_line = match.strip()
            clean_line = re.sub(r'\*\*\[.*?\]\*\*', '', clean_line)
            clean_line = re.sub(r'\[.*?\]', '', clean_line)
            clean_line = re.sub(r'\([^)]*\)', '', clean_line)
            clean_line = re.sub(r'\s+', ' ', clean_line).strip()
            if clean_line and len(clean_line) > 3:
                child_dialogue.append(clean_line)
        
        if child_dialogue:
            result = " ".join(child_dialogue) 
            logger.info(f"üé§ Extracted CHILD dialogue: {result}")
            return result
        
        # FINAL FALLBACK: Extract any meaningful quoted content
        lines = script.split('\n')
        meaningful_lines = []
        
        for line in lines:
            # Skip technical lines completely
            if any(skip in line.upper() for skip in ['===', 'GENERATED WITH', 'VIDEO ID', 'SOUND OF', 'SFX:', 'MUSIC', 'VISUAL:', '**(', 'TOTAL TIME']):
                continue
            
            # Look for quoted dialogue
            quoted_matches = re.findall(r'"([^"]+)"', line)
            for quote in quoted_matches:
                if len(quote) > 10:
                    meaningful_lines.append(quote)
        
        if meaningful_lines:
            result = " ".join(meaningful_lines[:3])
            logger.info(f"üé§ Extracted quoted dialogue: {result}")
            return result
        
        logger.warning("No clean dialogue found in script")
        return ""
    
    def _parse_time_to_seconds(self, time_str: str) -> float:
        """Parse time string to seconds (handles various formats)"""
        time_str = time_str.strip().lower()
        
        # Handle text-based formats like "2 seconds", "30s", "1 minute"
        import re
        
        # Extract numbers and time units
        if 'second' in time_str or 's' in time_str:
            numbers = re.findall(r'\d+\.?\d*', time_str)
            if numbers:
                return float(numbers[0])
        
        if 'minute' in time_str or 'm' in time_str:
            numbers = re.findall(r'\d+\.?\d*', time_str)
            if numbers:
                return float(numbers[0]) * 60
        
        if ':' in time_str:
            # Handle mm:ss format
            parts = time_str.split(':')
            if len(parts) == 2:
                minutes = float(parts[0])
                seconds = float(parts[1])
                return minutes * 60 + seconds
            else:
                # Handle potential h:mm:ss format
                return float(parts[-1])  # Just use seconds part
        
        # Try to extract just the number
        numbers = re.findall(r'\d+\.?\d*', time_str)
        if numbers:
            return float(numbers[0])
        
        # Fallback to 0
        return 0.0
        
    def _get_optimal_duration(self, platform: Platform) -> int:
        """Get optimal video duration for platform, respecting VIDEO_DURATION env var"""
        # Check for VIDEO_DURATION environment variable first
        env_duration = os.getenv('VIDEO_DURATION')
        if env_duration:
            try:
                duration = int(env_duration)
                logger.info(f"üéØ Using VIDEO_DURATION environment variable: {duration}s")
                return duration
            except ValueError:
                logger.warning(f"Invalid VIDEO_DURATION value: {env_duration}, using platform defaults")
        
        # Platform-specific defaults
        duration_map = {
            Platform.YOUTUBE: 30,  # YouTube Shorts can be up to 60s
            Platform.TIKTOK: 15,   # TikTok optimal is 15-30s
            Platform.INSTAGRAM: 30, # Instagram Reels up to 90s
            Platform.FACEBOOK: 30   # Facebook videos 15-60s
        }
        
        default_duration = duration_map.get(platform, 30)
        logger.info(f"üéØ Using platform default duration for {platform.value}: {default_duration}s")
        return default_duration

    def _extract_clean_overlay_text(self, content: str, scene_index: int) -> str:
        """Extract clean, short text for overlays from scene content"""
        import re
        
        # Remove technical directions and extract main message
        clean = re.sub(r'\*\*.*?\*\*', '', content)  # Remove bold markers
        clean = re.sub(r'\[.*?\]', '', clean)  # Remove time markers
        clean = re.sub(r'VISUAL:.*?(?=\*|$)', '', clean, flags=re.MULTILINE)
        clean = re.sub(r'SOUND:.*?(?=\*|$)', '', clean, flags=re.MULTILINE)
        clean = re.sub(r'CUT.*?\.', '', clean)
        clean = re.sub(r'\s+', ' ', clean).strip()
        
        # Create short, engaging overlay text based on scene
        if scene_index == 0:
            return "Baby vs. The Ultimate Challenge! üë∂"
        elif scene_index == 1:
            return "Epic Fails Incoming... üòÇ"
        elif scene_index == 2:
            return "Plot Twist! ü§Ø"
        elif scene_index == 3:
            return "Victory Achieved! üèÜ"
        else:
            # For longer videos, create dynamic text
            short_text = clean[:30] + "..." if len(clean) > 30 else clean
            return short_text or f"Amazing Scene {scene_index + 1}!"

    def _create_placeholder_video_content(self, config: GeneratedVideoConfig, scene_index: int, duration: float):
        """Create better placeholder video content while we integrate Veo-2"""
        from moviepy.editor import ColorClip, TextClip, CompositeVideoClip
        import random
        
        width, height = 1080, 1920
        
        # Create animated background
        if scene_index == 0:
            # Opening scene - dramatic colors
            bg_color = (255, 69, 0)  # Red-orange
            overlay_text = "üé¨ Baby Challenge Begins!"
        elif scene_index == 1:
            # Action scene - vibrant colors  
            bg_color = (30, 144, 255)  # Dodger blue
            overlay_text = "üòÇ Epic Baby Fails!"
        elif scene_index == 2:
            # Climax - exciting colors
            bg_color = (255, 165, 0)  # Orange
            overlay_text = "ü§Ø Unexpected Twist!"
        else:
            # Victory - celebratory colors
            bg_color = (50, 205, 50)  # Lime green
            overlay_text = "üèÜ Baby Wins!"
        
        # Create solid background
        background = ColorClip(size=(width, height), color=bg_color, duration=duration)
        
        # Create engaging overlay text with better positioning
        try:
            main_text = TextClip(
                overlay_text,
                fontsize=80,
                color='white',
                font='Arial-Bold',
                stroke_color='black',
                stroke_width=3
            ).set_position('center').set_duration(duration)
            
            # Add subtitle
            subtitle = TextClip(
                f"Scene {scene_index + 1}",
                fontsize=40,
                color='white',
                font='Arial',
                stroke_color='black',
                stroke_width=2
            ).set_position(('center', height * 0.8)).set_duration(duration)
            
            return CompositeVideoClip([background, main_text, subtitle])
            
        except Exception as e:
            logger.warning(f"Could not create TextClip, using background only: {e}")
            return background
    
    def _add_text_overlays(self, video_clip, config: GeneratedVideoConfig, duration: float) -> VideoClip:
        """Add styled text overlays to video with proper positioning to prevent cutoff"""
        try:
            # Generate AI-powered text overlays
            overlay_texts = self._generate_ai_text_overlays(config, duration)
            
            if not overlay_texts:
                logger.warning("No text overlays generated")
                return video_clip
            
            logger.info(f"üé® AI generated {len(overlay_texts)} styled text overlays")
            
            # Create text clips with proper positioning
            text_clips = []
            video_width, video_height = video_clip.size
            
            for i, overlay in enumerate(overlay_texts):
                try:
                    text = overlay['text']
                    start_time = overlay['start_time']
                    end_time = overlay['end_time']
                    
                    # Convert times to float for proper handling
                    start_time_float = float(start_time) if isinstance(start_time, str) else start_time
                    end_time_float = float(end_time) if isinstance(end_time, str) else end_time
                    
                    # CRITICAL FIX: Ensure text fits within video bounds
                    # Calculate safe text area (leave margins)
                    safe_width = int(video_width * 0.85)  # 85% of video width
                    safe_height = int(video_height * 0.15)  # 15% of video height for text
                    
                    # Dynamic font size based on text length and video size
                    text_length = len(text)
                    if text_length < 30:
                        font_size = max(40, int(video_width / 25))
                    elif text_length < 50:
                        font_size = max(35, int(video_width / 30))
                    else:
                        font_size = max(30, int(video_width / 35))
                    
                    # Position text overlays at different heights to avoid overlap
                    if i == 0:
                        # Top overlay
                        position = ('center', video_height * 0.15)
                    elif i == 1:
                        # Middle overlay  
                        position = ('center', video_height * 0.5)
                    else:
                        # Bottom overlay
                        position = ('center', video_height * 0.8)
                    
                    # Create text clip with proper wrapping
                    text_clip = TextClip(
                        text,
                        fontsize=font_size,
                        color='white',
                        font='Arial-Bold',
                        stroke_color='black',
                        stroke_width=2,
                        size=(safe_width, safe_height),
                        method='caption'  # Enables text wrapping
                    ).set_position(position).set_start(start_time_float).set_end(end_time_float)
                    
                    text_clips.append(text_clip)
                    
                    logger.info(f"Added text overlay: '{text[:30]}...' at {start_time_float:.1f}s-{end_time_float:.1f}s")
                    
                except Exception as e:
                    logger.error(f"Error creating text overlay {i}: {e}")
                    continue
            
            if text_clips:
                # Composite all text clips with the video
                final_video = CompositeVideoClip([video_clip] + text_clips)
                logger.info("Successfully added {} text overlays to video".format(len(text_clips)))
                return final_video
            else:
                logger.warning("No text clips were successfully created")
                return video_clip
                
        except Exception as e:
            logger.error(f"Error adding text overlays: {e}")
            return video_clip
    
    def _generate_ai_text_overlays(self, config: GeneratedVideoConfig, duration: float) -> List[Dict]:
        """Generate AI-powered text overlays based on video content and style"""
        overlays = []
        
        try:
            # Use AI to generate text overlays based on video style and tone
            ai_prompt = f"""
            Create AI-powered text overlays for a {duration:.0f}-second video about: {config.topic}
            
            Video details:
            - Style: {config.style}
            - Tone: {config.tone}
            - Category: {config.category}
            
            Generate 3 text overlays in this exact JSON format:
            {{
                "overlay_1": {{
                    "text": "Engaging text with emoji",
                    "font": "Font name (e.g., Arial-Bold, Impact, Helvetica-Bold)",
                    "color": "Color name (e.g., yellow, cyan, orange, white)",
                    "position": "top, center, or bottom",
                    "timing": "start-end (e.g., 0-3)",
                    "size": "small, medium, or large"
                }},
                "overlay_2": {{"text": "Another overlay", "font": "Arial-Bold", "color": "white", "position": "center", "timing": "5-8", "size": "medium"}},
                "overlay_3": {{"text": "Final overlay", "font": "Impact", "color": "yellow", "position": "bottom", "timing": "10-13", "size": "large"}}
            }}
            
            Requirements:
            - Use emojis to make text more engaging
            - Suggest contrasting colors that pop on video
            - Font should match the tone (playful, serious, etc.)
            - First overlay appears at start
            - Last overlay should be a call-to-action
            - Keep text short and impactful
            
            Return ONLY the JSON, no other text.
            """
            
            response = self.refinement_model.generate_content(ai_prompt)
            
            import re
            overlay_data = self._extract_json_safely(response.text)
            if overlay_data:
                
                # Convert AI suggestions to overlay format
                for i, (key, overlay_info) in enumerate(overlay_data.items()):
                    timing = overlay_info.get('timing', '0-3').split('-')
                    start_time = float(timing[0]) if timing[0].isdigit() else 0
                    end_time = float(timing[1]) if len(timing) > 1 and timing[1].isdigit() else start_time + 3
                    
                    # Map size to fontsize
                    size_map = {'small': 50, 'medium': 65, 'large': 80}
                    fontsize = size_map.get(overlay_info.get('size', 'medium'), 65)
                    
                    # Map position - use relative positioning for better compatibility
                    position_map = {
                        'top': ('center', 0.15),
                        'center': 'center',
                        'bottom': ('center', 0.85)
                    }
                    position = position_map.get(overlay_info.get('position', 'center'), 'center')
                    
                    overlay = {
                        'text': overlay_info.get('text', f'Amazing Scene {i+1}!'),
                        'start_time': str(start_time),
                        'end_time': str(end_time),
                        'position': position,
                        'fontsize': fontsize,
                        'color': overlay_info.get('color', 'white'),
                        'font': overlay_info.get('font', 'Arial-Bold'),
                        'stroke_color': 'black',
                        'stroke_width': 3
                    }
                    
                    overlays.append(overlay)
                
                logger.info(f"üé® AI generated {len(overlays)} styled text overlays")
                return overlays
                
            else:
                logger.warning("AI text overlay generation failed, using defaults")
                
        except Exception as e:
            logger.error(f"Error with AI text overlay generation: {e}")
            
        # Fallback to default overlays if AI fails
        if "baby" in config.topic.lower() and "animal" in config.topic.lower():
            # Baby + animals specific overlays
            overlays = [
                {
                    'text': 'üë∂ Pure Joy!',
                    'start_time': '0',
                    'end_time': '3',
                    'position': ('center', 0.15),
                    'fontsize': 70,
                    'color': 'yellow',
                    'font': 'Arial-Bold'
                },
                {
                    'text': 'üê∂ Gentle Friends',
                    'start_time': str(duration * 0.4),
                    'end_time': str(duration * 0.7),
                    'position': 'center',
                    'fontsize': 60,
                    'color': 'white',
                    'font': 'Arial'
                },
                {
                    'text': '‚ù§Ô∏è Follow for More!',
                    'start_time': str(duration - 4),
                    'end_time': str(duration),
                    'position': ('center', 0.85),
                    'fontsize': 65,
                    'color': 'red',
                    'font': 'Impact'
                }
            ]
        else:
            # Generic engaging overlays with proper positioning
            overlays = [
                {
                    'text': 'üî• MUST WATCH!',
                    'start_time': '0',
                    'end_time': '3',
                    'position': ('center', 0.15),
                    'fontsize': 70,
                    'color': 'orange',
                    'font': 'Impact'
                },
                {
                    'text': 'ü§Ø AMAZING!',
                    'start_time': str(duration * 0.5),
                    'end_time': str(duration * 0.8),
                    'position': 'center',
                    'fontsize': 65,
                    'color': 'cyan',
                    'font': 'Arial-Bold'
                },
                {
                    'text': 'üëÜ FOLLOW FOR MORE',
                    'start_time': str(duration - 3),
                    'end_time': str(duration),
                    'position': ('center', 0.85),
                    'fontsize': 60,
                    'color': 'white',
                    'font': 'Helvetica-Bold'
                }
            ]
        
        logger.info(f"Generated {len(overlays)} text overlays for {duration:.1f}s video")
        return overlays
    
    def _create_video_analysis_file(self, config: GeneratedVideoConfig, script: str, 
                                   veo_clips: List[Dict], video_id: str, 
                                   generation_time: float, session_dir: str):
        """Create comprehensive video analysis file for the session"""
        try:
            analysis_path = os.path.join(session_dir, "video_analysis.txt")
            
            with open(analysis_path, 'w', encoding='utf-8') as f:
                f.write("üé¨ VIRAL VIDEO GENERATION ANALYSIS\n")
                f.write("=" * 60 + "\n\n")
                
                # Session Information
                f.write("üìã SESSION INFORMATION\n")
                f.write("-" * 30 + "\n")
                f.write(f"Video ID: {video_id}\n")
                f.write(f"Generation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Generation Time: {generation_time:.2f} seconds\n")
                f.write(f"Session Folder: {os.path.basename(session_dir)}\n\n")
                
                # Content Analysis
                f.write("üéØ CONTENT ANALYSIS\n")
                f.write("-" * 30 + "\n")
                f.write(f"Original Prompt: {config.topic}\n")
                f.write(f"Platform: {config.target_platform}\n") 
                f.write(f"Category: {config.category}\n")
                f.write(f"Duration: {config.duration_seconds} seconds\n")
                # Calculate aspect ratio based on platform
                aspect_ratio = "9:16" if str(config.target_platform).upper() == "TIKTOK" else "16:9"
                f.write(f"Aspect Ratio: {aspect_ratio}\n")
                f.write(f"Style: {config.style}\n")
                f.write(f"Tone: {config.tone}\n\n")
                
                # AI Models Used
                f.write("ü§ñ AI MODELS USED\n")
                f.write("-" * 30 + "\n")
                f.write("‚Ä¢ Gemini 2.5 Flash: Initial script generation\n")
                f.write("‚Ä¢ Gemini 2.5 Pro: Script refinement & TTS optimization\n")
                f.write("‚Ä¢ Google Veo-2: AI video clip generation\n")
                f.write("‚Ä¢ Google TTS: Natural voice synthesis\n\n")
                
                # Script Analysis
                f.write("üìù GENERATED SCRIPT ANALYSIS\n")
                f.write("-" * 30 + "\n")
                f.write(f"Script Length: {len(script)} characters\n")
                f.write(f"Estimated Reading Time: {len(script.split()) * 0.5:.1f} seconds\n")
                f.write("Content Type: ")
                if "baby" in config.topic.lower():
                    f.write("Baby content (family-friendly)\n")
                elif "animal" in config.topic.lower():
                    f.write("Animal content (pet-focused)\n")
                else:
                    f.write("General entertainment\n")
                f.write("\n")
                
                # Video Clips Analysis
                f.write("üé• VIDEO CLIPS ANALYSIS\n")
                f.write("-" * 30 + "\n")
                f.write(f"Total Clips Generated: {len(veo_clips)}\n")
                f.write(f"Prompt Quality Control: Gemini 2.5 Pro Reviewed\n\n")
                
                for i, clip in enumerate(veo_clips):
                    f.write(f"Clip {i+1}: {clip['description']}\n")
                    f.write(f"  Duration: {clip['duration']:.1f}s\n")
                    if os.path.exists(clip['clip_path']):
                        file_size = os.path.getsize(clip['clip_path']) / (1024*1024)
                        f.write(f"  File Size: {file_size:.1f}MB\n")
                    
                    # Include prompt refinement information
                    f.write(f"  Reviewed by: {clip.get('reviewed_by', 'none')}\n")
                    if 'original_prompt' in clip and clip['original_prompt'] != clip['veo2_prompt']:
                        f.write(f"  ORIGINAL PROMPT: {clip['original_prompt'][:100]}...\n")
                        f.write(f"  REFINED PROMPT: {clip['veo2_prompt'][:100]}...\n")
                    else:
                        f.write(f"  VEO-2 PROMPT: {clip['veo2_prompt'][:100]}...\n")
                    
                    if clip.get('is_fallback'):
                        f.write(f"  ‚ö†Ô∏è  FALLBACK CLIP (Veo-2 generation failed)\n")
                    f.write("\n")
                
                # Performance Metrics
                f.write("üìä PERFORMANCE METRICS\n")
                f.write("-" * 30 + "\n")
                f.write(f"Generation Speed: {config.duration_seconds/generation_time:.2f}s video per minute\n")
                f.write(f"Real Veo-2 Usage: {'Yes' if hasattr(config, 'use_real_veo2') and config.use_real_veo2 else 'No'}\n")
                f.write(f"Predicted Viral Score: {getattr(config, 'predicted_viral_score', 'N/A')}\n\n")
                
                # File Structure
                f.write("üìÅ SESSION FILES\n")
                f.write("-" * 30 + "\n")
                session_files = os.listdir(session_dir)
                for file in sorted(session_files):
                    if os.path.isfile(os.path.join(session_dir, file)):
                        file_path = os.path.join(session_dir, file)
                        file_size = os.path.getsize(file_path) / (1024*1024)
                        f.write(f"‚Ä¢ {file} ({file_size:.1f}MB)\n")
                f.write("\n")
                
                # Full Script
                f.write("üìú COMPLETE GENERATED SCRIPT\n")
                f.write("-" * 30 + "\n")
                f.write(script)
                f.write("\n\n")
                
                f.write("üéâ Analysis Complete!\n")
            
            logger.info(f"üìä Created video analysis file: {analysis_path}")
            
        except Exception as e:
            logger.error(f"Error creating video analysis file: {e}")
    
    def _add_emotional_context_to_tts(self, clean_script: str, narrative: str, feeling: str) -> str:
        """Add emotional context while keeping TTS clean and readable"""
        
        # CRITICAL: For TTS, we need clean text that sounds natural when spoken
        # NO emotional directions, NO stage directions, NO brackets
        
        try:
            # Clean the script thoroughly first
            import re
            
            # Remove all emotional directions that TTS might read aloud
            clean_script = re.sub(r'\[.*?\]', '', clean_script)  # Remove [chuckle in disbelief]
            clean_script = re.sub(r'\(.*?\)', '', clean_script)  # Remove (laughing)
            clean_script = re.sub(r'\*.*?\*', '', clean_script)  # Remove *emotional*
            clean_script = re.sub(r'{.*?}', '', clean_script)   # Remove {direction}
            clean_script = re.sub(r'<.*?>', '', clean_script)   # Remove <emotion>
            clean_script = re.sub(r'\s+', ' ', clean_script).strip()
            
            # For TTS, keep it simple and natural - just add minimal natural speech patterns
            if feeling == "excited" or feeling == "energetic":
                return f"Whoa! {clean_script} This is incredible!"
            elif feeling == "funny":
                return f"Okay, get this... {clean_script} I know, right?"
            elif feeling == "dramatic":
                return f"Listen... {clean_script} This changes everything."
            elif feeling == "emotional":
                return f"This is beautiful... {clean_script} Absolutely amazing."
            else:
                # For most cases, just use the clean script as-is
                return clean_script
            
        except Exception as e:
            logger.warning(f"Could not enhance TTS: {e}")
            # Always return clean script as fallback
            return clean_script
    
    def _extract_json_safely(self, text: str) -> Dict[str, Any]:
        """Safely extract JSON from AI response text with multiple fallback methods"""
        import re
        import json
        
        logger.debug(f"Attempting to extract JSON from response of length {len(text)}")
        
        # Method 1: Try direct JSON parsing
        try:
            result = json.loads(text)
            logger.debug("Successfully parsed JSON directly")
            return result
        except Exception as e:
            logger.debug(f"Direct JSON parsing failed: {e}")
            pass
        
        # Method 2: Extract JSON array specifically (for VEO2 prompts)
        try:
            # Look for array pattern
            array_match = re.search(r'\[\s*\{.*?\}\s*\]', text, re.DOTALL)
            if array_match:
                json_text = array_match.group()
                result = json.loads(json_text)
                logger.debug("Successfully extracted JSON array")
                return result
        except Exception as e:
            logger.debug(f"Array extraction failed: {e}")
            pass
        
        # Method 3: Extract JSON block with regex
        try:
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                json_text = json_match.group()
                result = json.loads(json_text)
                logger.debug("Successfully extracted JSON object")
                return result
        except Exception as e:
            logger.debug(f"Object extraction failed: {e}")
            pass
            
        # Method 4: Look for JSON between code blocks
        try:
            # Try ```json blocks
            json_match = re.search(r'```(?:json)?\s*(\[.*?\]|\{.*?\})\s*```', text, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
                result = json.loads(json_text)
                logger.debug("Successfully extracted JSON from code block")
                return result
        except Exception as e:
            logger.debug(f"Code block extraction failed: {e}")
            pass
            
        # Method 5: Clean and try parsing
        try:
            # Remove markdown formatting
            cleaned = re.sub(r'```(?:json)?', '', text)
            cleaned = re.sub(r'```', '', cleaned)
            cleaned = cleaned.strip()
            
            # Try to find JSON-like structure
            json_match = re.search(r'(\[.*?\]|\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})', cleaned, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
                result = json.loads(json_text)
                logger.debug("Successfully extracted JSON after cleaning")
                return result
        except Exception as e:
            logger.debug(f"Cleaned extraction failed: {e}")
            pass
            
        # Method 6: Fix common JSON issues and retry
        try:
            if '[' in text and ']' in text:
                # Extract array
                start = text.find('[')
                end = text.rfind(']') + 1
                json_candidate = text[start:end]
            elif '{' in text and '}' in text:
                # Extract object
                start = text.find('{')
                end = text.rfind('}') + 1
                json_candidate = text[start:end]
            else:
                raise ValueError("No JSON structure found")
                
            # Fix common issues
            json_candidate = re.sub(r',\s*}', '}', json_candidate)  # Remove trailing commas
            json_candidate = re.sub(r',\s*]', ']', json_candidate)   # Remove trailing commas in arrays
            json_candidate = re.sub(r'([^"\\])\n', r'\1', json_candidate)  # Remove line breaks in strings
            json_candidate = re.sub(r'\\n', ' ', json_candidate)  # Replace escaped newlines
            
            result = json.loads(json_candidate)
            logger.debug("Successfully parsed JSON after fixing common issues")
            return result
        except Exception as e:
            logger.debug(f"Final extraction attempt failed: {e}")
            pass
            
        logger.warning(f"All JSON extraction methods failed. Response preview: {text[:200]}...")
        return {}
    
    def _get_emotional_tts_settings(self, feeling: str) -> dict:
        """Get TTS settings optimized for specific emotional tone"""
        
        # Base settings for more natural voice
        base_settings = {
            'lang': 'en',
            'slow': False,
        }
        
        # Emotional TTS optimizations
        if feeling in ['serious', 'dramatic']:
            return {
                **base_settings,
                'tld': 'com',  # US English for authority
                'slow': False,
            }
        elif feeling in ['funny', 'playful', 'energetic']:
            return {
                **base_settings,
                'tld': 'co.uk',  # UK English for playful tone
                'slow': False,
            }
        elif feeling in ['inspirational', 'emotional']:
            return {
                **base_settings,
                'tld': 'com.au',  # Australian English for warmth
                'slow': False,
            }
        elif feeling in ['cynical', 'calm']:
            return {
                **base_settings,
                'tld': 'com',  # US English for neutral/cynical
                'slow': True,  # Slightly slower for emphasis
            }
        else:  # neutral, default
            return {
                **base_settings,
                'tld': 'co.uk',  # UK English as default
                'slow': False,
            }

    def _review_and_refine_veo2_prompts(self, veo_prompts: List[Dict], config: GeneratedVideoConfig) -> List[Dict]:
        """Use Gemini 2.5 Pro to review and refine Veo-2 prompts for realism and quality"""
        logger.info("üîç Gemini 2.5 Pro reviewing Veo-2 prompts for quality and realism...")
        
        refined_prompts = []
        
        for i, prompt_data in enumerate(veo_prompts):
            try:
                logger.info(f"üìù Reviewing prompt {i+1}/{len(veo_prompts)}")
                
                review_prompt = f"""
                You are a professional video production expert and cultural consultant. Review this Veo-2 video generation prompt and refine it for maximum realism and quality.
                
                ORIGINAL PROMPT:
                {prompt_data['veo2_prompt']}
                
                SCENE DESCRIPTION:
                {prompt_data['description']}
                
                VIDEO CONTEXT:
                - Topic: {config.topic}
                - Duration: {prompt_data['duration']:.1f} seconds
                - Target Platform: {config.target_platform}
                
                REVIEW CRITERIA:
                1. REALISM CHECK:
                   - Remove any unrealistic combinations (e.g., "lady soldier with Kipa" - inconsistent)
                   - Ensure culturally accurate and authentic
                   - Check for logical consistency in character descriptions
                   - Verify setting and context make sense together
                
                2. DETAIL ENHANCEMENT:
                   - Add specific camera angles and movements
                   - Include lighting and atmosphere details
                   - Specify realistic clothing, expressions, and actions
                   - Add environmental details that enhance the scene
                
                3. TECHNICAL QUALITY:
                   - Ensure prompt is detailed enough for high-quality generation
                   - Add professional cinematography elements
                   - Specify video quality and style requirements
                   - Include timing and pacing details
                
                4. CULTURAL SENSITIVITY:
                   - Ensure culturally appropriate representations
                   - Remove stereotypes or unrealistic cultural elements
                   - Verify authentic context for the setting
                
                EXAMPLE OF GOOD REFINEMENT:
                BEFORE: "Lady soldier with Kipa in Israeli cafe"
                AFTER: "Young Israeli woman in casual civilian clothes sitting in a modern Tel Aviv cafe, natural sunlight streaming through windows, contemporary urban setting with authentic Middle Eastern atmosphere"
                
                Return the REFINED VEO-2 PROMPT that is:
                - Culturally accurate and realistic
                - Highly detailed for quality generation
                - Professional cinematography style
                - {prompt_data['duration']:.1f} seconds of compelling content
                
                Return ONLY the refined prompt text.
                """
                
                response = self.refinement_model.generate_content(review_prompt)
                refined_prompt = response.text.strip()
                
                # Clean the refined prompt
                import re
                refined_prompt = re.sub(r'\*+', '', refined_prompt)
                refined_prompt = re.sub(r'\s+', ' ', refined_prompt).strip()
                
                # Create refined prompt data
                refined_prompt_data = prompt_data.copy()
                refined_prompt_data['veo2_prompt'] = refined_prompt
                refined_prompt_data['reviewed_by'] = 'gemini-2.5-pro'
                
                refined_prompts.append(refined_prompt_data)
                
                logger.info(f"‚úÖ Refined prompt {i+1}: {refined_prompt[:100]}...")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to refine prompt {i+1}: {e}")
                logger.info(f"üîÑ Using original prompt for scene {i+1}")
                # Use original prompt if refinement fails
                refined_prompts.append(prompt_data)
        
        logger.info(f"‚úÖ Gemini 2.5 Pro review complete: {len(refined_prompts)} prompts refined")
        return refined_prompts

    def _get_tts_example(self, duration: int, target_words: int) -> str:
        """Generate a duration-appropriate TTS example"""
        examples = {
            5: "Whoa wait what? That's absolutely incredible, look at that!",  # 10 words
            10: "Oh my goodness, you're not going to believe this! Watch what happens next - it's mind-blowing and totally unexpected!",  # 20 words
            15: "Okay, hold on, did you see that? This is getting crazy! Look at how this develops - I've never seen anything quite like this before, seriously amazing!",  # 30 words
            20: "Alright everyone, pay attention because this is absolutely insane! The way this unfolds is just incredible. You're going to want to watch this multiple times to catch everything that's happening here. Trust me on this one!",  # 40 words
            30: "Oh wow, this is absolutely unbelievable! I can't even process what I'm seeing right now. The sheer scale of what's happening here is mind-blowing. Watch how this develops - every second gets more intense than the last. You're witnessing something truly special here, folks. This is the kind of content that changes everything!",  # 60 words
            45: "Okay everyone, buckle up because what you're about to see will blow your mind! This is hands down one of the most incredible things I've ever witnessed. The way this story unfolds is absolutely phenomenal - each moment builds on the last in ways you won't expect. Pay close attention to every detail because there's so much happening here. Seriously, I've watched this multiple times and I'm still discovering new things. This is why we love this kind of content - it never gets old!",  # 90 words
            60: "Alright, I need everyone to stop what they're doing and watch this right now! What you're about to witness is going to change your entire perspective. I'm not exaggerating - this is genuinely one of those moments that defines everything. From the very first second, you'll be hooked. The way this develops is unlike anything I've seen before. Every single frame tells a story, every moment has meaning. You'll want to share this with everyone you know because it's that impactful. I'm getting chills just thinking about it again. This is the kind of content that reminds us why we're here - pure, unfiltered amazingness that captures the human experience in ways words can't describe. Get ready for an emotional rollercoaster!",  # 120 words
        }
        
        # Find the closest example
        closest_duration = min(examples.keys(), key=lambda x: abs(x - duration))
        example = examples[closest_duration]
        
        # Trim or extend to match target words
        words = example.split()
        if len(words) > target_words:
            return ' '.join(words[:target_words])
        else:
            # Add padding words if needed
            padding = ["Amazing!", "Incredible!", "Wow!", "Look!", "See that?", "Unbelievable!"]
            while len(words) < target_words:
                words.append(padding[len(words) % len(padding)])
            return ' '.join(words[:target_words])

    def generate_voice_over(self, tts_script: str, config: Dict = None, session_dir: str = None) -> str:
        """Generate voice-over for the TTS script"""
        if not config:
            config = {}
            
        feeling = config.get('feeling', 'neutral')
        narrative = config.get('narrative', 'neutral')
        realistic_audio = config.get('realistic_audio', False)
        duration_target = config.get('duration_seconds', 30)
        
        logger.info(f"Generating voice-over: {feeling} emotion, {narrative} perspective")
        
        # Try Google Cloud TTS first if realistic_audio is requested
        if realistic_audio:
            try:
                from .google_tts_client import GoogleTTSClient
                
                logger.info("üé§ Using Google Cloud TTS for natural voice...")
                google_tts = GoogleTTSClient()
                audio_path = google_tts.generate_speech(
                    text=tts_script,
                    feeling=feeling,
                    narrative=narrative,
                    duration_target=duration_target
                )
                
                if audio_path and os.path.exists(audio_path):
                    # Move to session directory
                    final_path = os.path.join(session_dir, f"google_tts_{uuid.uuid4()}.mp3")
                    os.rename(audio_path, final_path)
                    logger.info(f"‚úÖ Google Cloud TTS generated: {final_path}")
                    return final_path
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Google Cloud TTS failed: {e}")
                logger.info("üîÑ Falling back to basic gTTS...")
        
        # Enhanced TTS script with emotion
        enhanced_script = self._enhance_tts_script(tts_script, feeling, narrative)

    def _get_scene_type(self, scene_index: int, total_clips: int) -> str:
        """Determine scene type based on position in video"""
        if scene_index == 0:
            return "Opening"
        elif scene_index == total_clips - 1:
            return "Conclusion"
        elif scene_index == 1:
            return "Introduction"
        elif scene_index == total_clips - 2:
            return "Climax"
        else:
            return "Main action"

    def _generate_fallback_veo2_prompt(self, topic: str, scene_type: str, duration: float) -> str:
        """Generate a fallback Veo-2 prompt for any topic and scene type"""
        return f"""Create a REALISTIC video for: {topic}
{scene_type} scene with professional quality content.
Natural lighting, smooth camera work, engaging visuals.
Show {scene_type.lower()} content that advances the story.
Duration: {duration:.1f} seconds."""