"""Universal news scraper with hardcoded test data for now"""

import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import random

from ...utils.logging_config import get_logger
from ..models.content_models import (
    ContentItem, NewsSource, MediaAsset, 
    AssetType, SourceType, ContentStatus
)
from .test_media_scraper import TestMediaScraper

logger = get_logger(__name__)


class TestUniversalNewsScraper:
    """Universal scraper that can scrape any news site"""
    
    def __init__(self):
        self.media_scraper = TestMediaScraper()
        # Test news data - Hebrew news
        self.test_data = {
            "ynet": [
                {
                    "title": "转  专: '爪 专砖 转 拽砖转'",
                    "content": "专砖 砖 转 砖   专 住转 专  爪  专砖 拽转 转 拽砖转. 砖专  住注专 专砖 专转  爪注 转. 驻爪 转 拽专转 专驻 注 驻 砖专.",
                    "image": "https://ynet-pic1.yit.co.il/cdn-cgi/image/format=auto/picserver6/crop_images/2025/07/14/r1lVEGzUlx/r1lVEGzUlx_0_0_850_479_0_x-large.jpg",
                    "video": None
                },
                {
                    "title": "  拽爪: 45 注转 爪 转",
                    "content": "  拽爪 驻拽 转 砖专. 转  45 注转 爪, 砖 砖  . 砖专 专转 专.",
                    "image": "https://ynet-pic1.yit.co.il/cdn-cgi/image/format=auto/picserver6/crop_images/2025/08/05/SklEHLiJOeg/SklEHLiJOeg_0_136_1280_718_0_medium.jpg",
                    "video": None
                },
                {
                    "title": " 转  转 驻转 专驻 专住",
                    "content": "爪 专  转  砖专 注 专 专 89-87 转 注 专驻.   专转.",
                    "image": "https://ynet-pic1.yit.co.il/cdn-cgi/image/format=auto/picserver6/crop_images/2022/12/15/HJHFUxFOi/HJHFUxFOi_0_0_850_479_0_medium.jpg",
                    "video": None
                },
                {
                    "title": "转转 专转: 注专 转 3000 砖 砖驻 ",
                    "content": "拽专 专住转  专 砖驻 注专 注转拽 转 3000 砖 . 爪   专住 转转 注转拽转.",
                    "image": "https://ynet-pic1.yit.co.il/cdn-cgi/image/format=auto/picserver6/crop_images/2025/07/30/BJ11S2ySDPge/BJ11S2ySDPge_0_187_2000_1130_0_medium.jpg",
                    "video": None
                },
                {
                    "title": "拽: 专 砖专转 专 -500  专",
                    "content": "专转 住专 砖专转 CyberShield 专砖 注  注拽转  专拽转 -500  专.",
                    "image": "https://ynet-pic1.yit.co.il/cdn-cgi/image/format=auto/picserver6/crop_images/2025/07/31/H1QLVFhOPxl/H1QLVFhOPxl_0_77_1500_845_0_medium.jpg",
                    "video": None
                }
            ],
            "mako": [
                {
                    "title": "住注专 住转: \" 砖注 ",
                    "content": " 住注专 住转 住转 砖注转 砖砖 专 住转. \"专 住转 抓 驻住拽 转 砖.",
                    "image": "https://img.mako.co.il/2024/08/05/knesset_storm.jpg",
                    "video": None
                },
                {
                    "title": "拽专: 拽驻 驻转 住 转 ",
                    "content": "拽专 砖  砖砖转转 3 住转 拽驻  驻转 -20% 转 住 转 .",
                    "image": "https://img.mako.co.il/2024/08/04/coffee_health.jpg",
                    "video": None
                }
            ],
            "sport5": [
                {
                    "title": " 住 注 砖专 砖拽 转",
                    "content": " 专 注 注 专转 专 砖拽 转  砖专 专.",
                    "image": "https://sport5.co.il/uploads/2024/08/messi_israel.jpg",
                    "video": None
                }
            ]
        }
    
    async def scrape(self, source: NewsSource, hours_back: int = 24) -> List[ContentItem]:
        """Scrape news from source"""
        logger.info(f"Scraping {source.url} (last {hours_back} hours)")
        
        # Determine source type
        source_key = None
        if "ynet" in source.url:
            source_key = "ynet"
        elif "mako" in source.url:
            source_key = "mako"
        elif "sport5" in source.url:
            source_key = "sport5"
        
        if not source_key or source_key not in self.test_data:
            logger.warning(f"No test data for {source.url}")
            return []
        
        # Get test articles
        articles = []
        now = datetime.now()
        
        for idx, data in enumerate(self.test_data[source_key]):
            # Create time within hours_back
            published = now - timedelta(hours=random.randint(1, min(hours_back, 23)))
            
            # Create media assets
            media_assets = []
            if data["image"]:
                media_assets.append(MediaAsset(
                    id=f"{source_key}_{idx}_image",
                    asset_type=AssetType.IMAGE,
                    source_url=data["image"]
                ))
            
            if data.get("video"):
                media_assets.append(MediaAsset(
                    id=f"{source_key}_{idx}_video",
                    asset_type=AssetType.VIDEO,
                    source_url=data["video"]
                ))
            
            # Create content item
            article = ContentItem(
                id=f"{source_key}_{idx}_{published.timestamp()}",
                source=source,
                title=data["title"],
                content=data["content"],
                media_assets=media_assets,
                published_date=published,
                language="he",
                author=source.name,
                url=f"{source.url}/article/{idx}",
                status=ContentStatus.SCRAPED,
                categories=["news"],
                metadata={
                    "scraped_at": datetime.now().isoformat(),
                    "source_type": source_key
                }
            )
            
            articles.append(article)
        
        logger.info(f"Scraped {len(articles)} articles from {source.name}")
        return articles


class TelegramChannelScraper:
    """Scraper for Telegram channels"""
    
    def __init__(self):
        # Test Telegram data
        self.test_channels = {
            "@ynet_news": [
                {
                    "title": "★ 祝: 驻爪抓 拽 砖注 转 ",
                    "content": "驻爪抓 拽 砖注 驻 拽转 住驻专转 专 转 . 转 爪 专 拽.",
                    "image": "https://ynet-pic1.yit.co.il/picserver5/wcm_upload/2024/08/05/tel_aviv_explosion.jpg",
                    "video": None
                }
            ],
            "@channel13news": [
                {
                    "title": " 砖专 : 住转 注转 砖 专砖 砖",
                    "content": "专砖 砖 拽 注 拽转 住驻专转 住转 注转 砖 爪 .",
                    "image": None,
                    "video": "https://test-streams.mux.dev/x36xhzz/x36xhzz.m3u8"
                }
            ]
        }
    
    async def scrape_channel(self, channel_name: str, hours_back: int = 24) -> List[ContentItem]:
        """Scrape Telegram channel"""
        logger.info(f"Scraping Telegram channel {channel_name}")
        
        if channel_name not in self.test_channels:
            logger.warning(f"No test data for channel {channel_name}")
            return []
        
        articles = []
        now = datetime.now()
        
        # Create fake source
        source = NewsSource(
            id=f"telegram_{channel_name}",
            name=f"Telegram: {channel_name}",
            url=f"https://t.me/{channel_name.replace('@', '')}",
            source_type=SourceType.TELEGRAM
        )
        
        for idx, data in enumerate(self.test_channels[channel_name]):
            # Create recent timestamp
            published = now - timedelta(minutes=random.randint(5, hours_back * 60))
            
            # Create media assets
            media_assets = []
            if data.get("image"):
                media_assets.append(MediaAsset(
                    id=f"telegram_{channel_name}_{idx}_image",
                    asset_type=AssetType.IMAGE,
                    source_url=data["image"]
                ))
            
            if data.get("video"):
                media_assets.append(MediaAsset(
                    id=f"telegram_{channel_name}_{idx}_video",
                    asset_type=AssetType.VIDEO,
                    source_url=data["video"]
                ))
            
            # Create content item
            article = ContentItem(
                id=f"telegram_{channel_name}_{idx}_{published.timestamp()}",
                source=source,
                title=data["title"],
                content=data["content"],
                media_assets=media_assets,
                published_date=published,
                language="he",
                author=channel_name,
                url=f"{source.url}/{idx}",
                status=ContentStatus.SCRAPED,
                categories=["telegram", "breaking"],
                metadata={
                    "channel": channel_name,
                    "is_forwarded": False,
                    "views": random.randint(1000, 50000)
                }
            )
            
            articles.append(article)
        
        logger.info(f"Scraped {len(articles)} messages from {channel_name}")
        return articles